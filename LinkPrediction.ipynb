{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_org_data = 'data/original/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPrediction(object):\n",
    "    \n",
    "    def __init__(self, data_path, df_merge_file, df_nodes_file, df_relations_file, emb_file, light_file, dark_file, test_set, result_path):\n",
    "        self.__data_path         = data_path\n",
    "        self.__df_relations_file = df_relations_file\n",
    "        self.__df_nodes_file     = df_nodes_file\n",
    "        self.__df_merge_file     = df_merge_file \n",
    "        self.__emb_file          = emb_file\n",
    "        self.__light_file        = light_file\n",
    "        self.__dark_file         = dark_file\n",
    "        self.__result_path       = result_path\n",
    "        self.__test_set          = test_set\n",
    "        \n",
    "        \n",
    "        self.__merge_cols        = ['h','t','r','h_id','t_id', 'r_id']\n",
    "        self.__merge_cols_names  = self.__merge_cols[:3]\n",
    "        self.__merge_cols_ids    = self.__merge_cols[3:]\n",
    "        self.__other_cols        = ['id', 'name']\n",
    "        \n",
    "        self.__load_files()\n",
    "        self.__read_embeddings()\n",
    "        self.__get_training_examples()\n",
    "        \n",
    "    def draw_pca_of_all_nodes(self):\n",
    "        self.__label_all_node_embedding()\n",
    "        self.__plot_pca (self.__labeled_emb, 'PCA-{}.png'.format(self.__emb_file))\n",
    "        \n",
    "    def train_model_cross_validation(self, solver, multi_class, C=1e5, cv = 10, sel_feat = False, ver = ''):\n",
    "        \n",
    "        lst_X = [i[:-1] for i in self.__lst_all]\n",
    "            \n",
    "        lst_Y = [i[-1:] for i in self.__lst_all]\n",
    "        \n",
    "        print('train_model_cross_validation| train_model| {} and {}'.format(len(lst_X[0]), len(lst_Y[0])))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(lst_X, lst_Y, test_size=0.3, random_state=0)\n",
    "        print('train_model_cross_validation| train_model| X_train: {}, X_test: {}, y_train: {}, y_test:{}'.format(len(X_train), len(X_test), len(y_train), len(y_test)))\n",
    "        \n",
    "        y_train = np.asarray(y_train)\n",
    "        \n",
    "        self.logreg = LogisticRegression(C=C, solver=solver, multi_class=multi_class)\n",
    "        y_score_lr = self.logreg.fit(X_train, np.ravel(y_train)).decision_function(X_test)\n",
    "        \n",
    "        scores = cross_val_score(self.logreg, X_train, np.ravel(y_train), cv=cv)\n",
    "        print('train_model_cross_validation| scores: {}'.format(scores ))\n",
    "        print(\"train_model_cross_validation| Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "        \n",
    "#         y_score_lr = self.logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "        \n",
    "        p, r, th = self.__precision_recall_curve(y_test ,y_score_lr, ver)  \n",
    "        \n",
    "        \n",
    "        self.__AUC_ROC(y_test ,y_score_lr, ver)\n",
    "        \n",
    "        y_pred = self.logreg.predict(X_test)\n",
    "        print('train_model_cross_validation| Accuracy of logistic regression classifier on test set: {:.2f}'.format(self.logreg.score(X_test, y_test)))\n",
    "        confusionmatrix = confusion_matrix(y_test, y_pred)\n",
    "        print(confusionmatrix)\n",
    "        \n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "    def test_dark_kinase(self, sel_feat):\n",
    "        self.__get_dark_kinase_emb() # create lst_emb_dark_pathway\n",
    "        self.__prepare_dark_emb() # create lst_dark_all\n",
    "        test =  self.__test(self.__lst_dark_all, sel_feat) # uses lst_dark_all\n",
    "        \n",
    "    def evaluate_test_set(self, sel_feat):\n",
    "        self.__load_test_set(self.__test_set)\n",
    "        self.__get_test_set_emb(self.__df_test_set)\n",
    "        self.__prepare_test_set_emb()\n",
    "        test =  self.__test(self.__lst_test_set_all, sel_feat)\n",
    "        \n",
    "    def __prepare_test_set_emb(self):\n",
    "        \n",
    "        list_test_set_yes_emb = [ [i[1], i[2]] for i in self.__lst_test_set]\n",
    "        self.__lst_test_set_all = list()\n",
    "        for i in list_test_set_yes_emb:\n",
    "            lst = list([])\n",
    "            lst.extend(i[0])\n",
    "            lst.append(i[1])\n",
    "            self.__lst_test_set_all.append(lst)\n",
    "        \n",
    "        print('__prepare_test_set_emb| list of test set prepared {}'.format(len(self.__lst_test_set_all)))\n",
    "        \n",
    "    def __get_test_set_emb(self, df):\n",
    "        err = 0\n",
    "        self.__lst_test_set = list()\n",
    "        \n",
    "        for i in df.itertuples():\n",
    "            try:\n",
    "                indx = i[0]\n",
    "                h_id = i[4]\n",
    "                t_id = i[5]\n",
    "                h_emb = self.__emb_dict[str(h_id)]\n",
    "                t_emb = self.__emb_dict[str(t_id)]\n",
    "                emb = np.multiply(h_emb, t_emb)\n",
    "                self.__lst_test_set.append([indx, emb, 1])\n",
    "            except KeyError :\n",
    "                err+=1\n",
    "        print('__get_test_set_emb| test set emb list : {} with {} number of errors.'.format(len(self.__lst_test_set), err))\n",
    "        \n",
    "    def __load_test_set(self, test_file):\n",
    "        mcols = self.__merge_cols\n",
    "        dtype = {mcols[0]:object, mcols[1]:object, mcols[2]:object, mcols[3]:int, mcols[4]:int, mcols[5]:int}\n",
    "        self.__df_test_set     = pd.read_csv(self.__data_path + test_file ,dtype=dtype)\n",
    "        print('__load_test_set: df_test_set: {}'.format(self.__df_test_set.shape))\n",
    "        \n",
    "    def generate_dict_possible_pairs(self):\n",
    "        \n",
    "        self.__get_df_dark_node()\n",
    "        self.__get_df_pathway_node()\n",
    "        self.__get_df_human_pathway_node()\n",
    "        print('generate_dict_possible_pairs| df_dark_node: {}, self.__df_human_pathway_node: {}'.format(self.__df_dark_node.shape[0], self.__df_human_pathway_nodes.shape[0]))\n",
    "        \n",
    "        \n",
    "        self.__dict_pp_id     = dict()\n",
    "        self.__dict_ppid_emb  = dict()\n",
    "        self.__dict_ppid_pred = dict()\n",
    "        self.__dict_ppid_sig  = dict()\n",
    "        \n",
    "        index = 0\n",
    "        \n",
    "        for d in self.__df_dark_node.itertuples():\n",
    "            dark_id = d[1]\n",
    "            dark_emb = self.__emb_dict[str(d[1])]\n",
    "            \n",
    "            for p in self.__df_human_pathway_nodes.itertuples():\n",
    "                if str(p[1]) in self.__emb_dict:\n",
    "                    path_id  = p[1]\n",
    "                    path_emb = self.__emb_dict[str(p[1])]\n",
    "                    \n",
    "                    self.__dict_pp_id[(dark_id, path_id)] = index\n",
    "                    self.__dict_id_pp[index] = (dark_id, path_id)\n",
    "                    self.__dict_ppid_emb[index]  = np.multiply(dark_emb, path_emb)\n",
    "                    self.__dict_ppid_pred[index] = list()\n",
    "                    self.__dict_ppid_sig[index] = 0\n",
    "                    index+=1\n",
    "        \n",
    "        print('generate_dict_possible_pairs| self.__dict_pp_id: {}, self.__dict_ppid_emb: {}, self.__dict_ppid_pred: {}'\n",
    "              .format(len(self.__dict_pp_id), len(self.__dict_ppid_emb),len(self.__dict_ppid_pred)))\n",
    "        print()\n",
    "        \n",
    "    def get_prediction_for_all(self,path_org_data, gene_map_file, reactome_map_file, n_iter = 1, p = 0.5, alternative='two-sided', pvalue = 0.05):\n",
    "        \n",
    "        self.__path_org_data, self.__gene_map_file, self.__reactome_map_file = path_org_data, gene_map_file, reactome_map_file\n",
    "        \n",
    "        # generate id for each possible pairs\n",
    "        self.generate_dict_possible_pairs()\n",
    "        \n",
    "        c=0\n",
    "        # add prediction for each pair in dictionary\n",
    "        dict_ppid_emb = self.__dict_ppid_emb\n",
    "        for i in tqdm(range(n_iter)):\n",
    "            for k in dict_ppid_emb:\n",
    "                \n",
    "                prediction = self.logreg.predict_proba([dict_ppid_emb[k]])\n",
    "                self.__dict_ppid_pred[k].append(prediction)\n",
    "        \n",
    "        dict_pair_prob = dict()\n",
    "        for i in self.__dict_ppid_pred:\n",
    "            pair = self.__dict_id_pp[i]\n",
    "            dict_pair_prob[pair] = self.__dict_ppid_pred[i]\n",
    "        \n",
    "        self.save_dict_to_csv(dict_pair_prob, self.__emb_file)\n",
    "        \n",
    "    def save_dict_to_csv(self, d, file_name):\n",
    "        w = csv.writer(open(self.__data_path + self.__result_path + 'dict_pred_prob_{}'.format(file_name), \"w\"))\n",
    "        for key, val in d.items():\n",
    "            w.writerow([key, val])\n",
    "    \n",
    "    def statistical_significance_for_each_prediction(self,path_org_data, gene_map_file, reactome_map_file, n_iter = 1, p = 0.5, alternative='two-sided', pvalue = 0.05):\n",
    "        \n",
    "        self.__path_org_data, self.__gene_map_file, self.__reactome_map_file = path_org_data, gene_map_file, reactome_map_file\n",
    "        \n",
    "        # generate id for each possible pairs\n",
    "        self.generate_dict_possible_pairs()\n",
    "        \n",
    "        c=0\n",
    "        # add prediction for each pair in dictionary\n",
    "        dict_ppid_emb = self.__dict_ppid_emb\n",
    "        for i in tqdm(range(n_iter)):\n",
    "            for k in dict_ppid_emb:\n",
    "                \n",
    "                prediction = self.logreg.predict([dict_ppid_emb[k]])\n",
    "                if prediction == 1:\n",
    "                    c+=1\n",
    "                \n",
    "#                 prob = self.logreg.predict_proba([v])\n",
    "                self.__dict_ppid_pred[k].append(prediction[0])\n",
    "        print('c: {}'.format(c))\n",
    "        \n",
    "        \n",
    "        # test the significance\n",
    "        dict_ppid_pred = self.__dict_ppid_pred\n",
    "        for k in dict_ppid_pred:\n",
    "            success = dict_ppid_pred[k].count(1)\n",
    "            \n",
    "            if binom_test(success,n_iter,p = p, alternative = alternative) < pvalue:\n",
    "                self.__dict_ppid_sig[k] = 1\n",
    "                \n",
    "        \n",
    "        return self.__dict_ppid_sig.values()\n",
    "        \n",
    "    def predict_dark_pathway(self, df_all_merge, path_org_data, gene_map_file, reactome_map_file):\n",
    "        \n",
    "        \n",
    "        self.__path_org_data, self.__gene_map_file, self.__reactome_map_file = path_org_data, gene_map_file, reactome_map_file\n",
    "\n",
    "        self.__get_df_dark_node()\n",
    "        df_dark_node = self.__df_dark_node\n",
    "        \n",
    "        # after changing to train-test split we need to predict using merge_all\n",
    "        # self.__get_df_pathway_node()\n",
    "        # self.__get_df_human_pathway_node()\n",
    "        print('predict_dark_pathway| prediction process for train-test split...')\n",
    "        mcols = self.__merge_cols\n",
    "        dtype = {mcols[0]:object, mcols[1]:object, mcols[2]:object, mcols[3]:int, mcols[4]:int, mcols[5]:int}\n",
    "        df_all_merge = pd.read_csv(self.__data_path + df_all_merge ,dtype=dtype)\n",
    "        print('predict_dark_pathway| df_all_merge: {}'.format(df_all_merge.shape))\n",
    "        \n",
    "        df_p = df_all_merge[df_all_merge[mcols[2]] == 'hasPathway']\n",
    "        print('predict_dark_pathway| df_p: {}'.format(df_p.shape))\n",
    "        \n",
    "\n",
    "        list_unique_pathway = list(df_p[mcols[1]].unique())\n",
    "        \n",
    "\n",
    "        print('predict_dark_pathway| list_unique_pathway: {}'.format(len(list_unique_pathway)))\n",
    "        \n",
    "        cols = self.__other_cols        \n",
    "        df_pathway_node = self.__df_nodes[self.__df_nodes[cols[1]].isin(list_unique_pathway)]\n",
    "               \n",
    "        print('predict_dark_pathway| df_pathway_node: {}'.format(df_pathway_node.shape[0])) \n",
    "\n",
    "        \n",
    "        dict_reactome_ALL_name, dict_reactome_HSA_name = self.__generate_reactome_to_name(self.__path_org_data, self.__reactome_map_file)\n",
    "        print('predict_dark_pathway| dict_reactome_ALL_name: {}, dict_reactome_HSA_name: {}'.format(len(dict_reactome_ALL_name), len(dict_reactome_HSA_name)))\n",
    "        list_HSA      = list(dict_reactome_HSA_name.keys())\n",
    "        list_HSA_code = [i[i.rindex('-')+1:] for i in list_HSA]\n",
    "        print('predict_dark_pathway| list_HSA: {}, list_HSA_code: {}'.format(len(list_HSA), len(list_HSA_code)))\n",
    "\n",
    "        df_human_pathway_nodes = df_pathway_node[df_pathway_node['name'].isin(list_HSA_code)]\n",
    "        print('predict_dark_pathway| df_human_pathway_nodes: {}'.format(df_human_pathway_nodes.shape))\n",
    "        \n",
    "        print('predict_dark_pathway| Done prediction process for train-test split')\n",
    "        \n",
    "        #######################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        print('__predict_dark_pathway| df_dark_node: {}, self.__df_human_pathway_node: {}'.format(df_dark_node.shape[0], df_human_pathway_nodes.shape[0]))\n",
    "        \n",
    "        result = []\n",
    "        all_results_prob = []\n",
    "        \n",
    "        all_count = 0 \n",
    "        predicted_false_count = 0\n",
    "        predicted_true_count = 0\n",
    "        \n",
    "        cpro = 0\n",
    "        cpath = 0\n",
    "        \n",
    "        for d in df_dark_node.itertuples(): \n",
    "            all_count+=1\n",
    "            if str(d[1]) in self.__emb_dict:\n",
    "                \n",
    "                dark_emb = self.__emb_dict[str(d[1])]\n",
    "                for p in df_human_pathway_nodes.itertuples() :\n",
    "                    \n",
    "                    if str(p[1]) in self.__emb_dict:\n",
    "                        path_emb = self.__emb_dict[str(p[1])]\n",
    "                        e = np.multiply(dark_emb, path_emb)\n",
    "                        prediction = self.logreg.predict([e])\n",
    "                        prob = self.logreg.predict_proba([e])\n",
    "                        all_results_prob.append([d[2], p[2], prob[0][1]])\n",
    "                        \n",
    "                        if (prediction ==1):\n",
    "                            result.append([d[2], p[2], prob])\n",
    "                            predicted_true_count += 1\n",
    "                        else:\n",
    "                            predicted_false_count+=1\n",
    "                    else:\n",
    "                        cpath+=1\n",
    "#                         print('__predict_dark_pathway| missing pathway id from emb: {}'.format(d[1]))\n",
    "#                         break\n",
    "            else:\n",
    "#                 print('__predict_dark_pathway| missing protein id form emb: {}'.format(d[1]))\n",
    "                cpro+=1\n",
    "        print('__predict_dark_pathway| missing protein: {}, missing pathways: {}'.format(cpro, cpath))\n",
    "        \n",
    "#         return result , predicted_false_count, predicted_true_count\n",
    "        print('__predict_dark_pathway| possitive predictions: {} (negative predictions: {})'.format(len(result), predicted_false_count))\n",
    "        result.sort(key=lambda elem: elem[2][0][1], reverse=True)\n",
    "        predict_results_list = [(i[0], i[1],i[2][0][1]) for i in result]\n",
    "        predict_results_list = sorted(predict_results_list, key=itemgetter(2), reverse=True)\n",
    "        \n",
    "        print('__predict_dark_pathway| removing the known predictions...')\n",
    "        cols = self.__merge_cols\n",
    "        list_prediction_for_unknown = list()\n",
    "        for i in predict_results_list:\n",
    "            if (self.__df_dark_pathway[(self.__df_dark_pathway[cols[0]] == i[0]) & (self.__df_dark_pathway[cols[1]] == i[1])].shape[0] == 0):\n",
    "                list_prediction_for_unknown.append(i)\n",
    "        remained = len(predict_results_list) - len(list_prediction_for_unknown)\n",
    "        print('__predict_dark_pathway| all prediction: {}, prediction for unkowns: {}, remained(known): {}'.format(len(predict_results_list), len(list_prediction_for_unknown), remained))\n",
    "        \n",
    "        self.__list_prediction_for_unknown = list_prediction_for_unknown\n",
    "        self.__save_list_3cols(list_prediction_for_unknown, self.__emb_file)\n",
    "        print('__predict_dark_pathway| Raw Predictions saved.')\n",
    "        print()\n",
    "                                            \n",
    "        \n",
    "        df_all_results_prob = pd.DataFrame(all_results_prob, columns=['protein', 'pathway', 'probability'])\n",
    "        all_resutls_path_name = self.__data_path + self.__result_path + 'pred_all_prob_{}.csv'.format(self.__emb_file)\n",
    "        df_all_results_prob.to_csv(all_resutls_path_name, index=False)\n",
    "        print('__predict_dark_pathway| all predictions {} saved.'.format(df_all_results_prob.shape))\n",
    "                                                 \n",
    "        return pd.DataFrame(list_prediction_for_unknown, columns=['protein', 'pathway', 'probability']), predict_results_list\n",
    "    \n",
    "    def generating_full_prediction_list(self):\n",
    "        df_prediction_raw_unkowns =pd.DataFrame(self.__list_prediction_for_unknown, columns=['protein','pathway', 'score'])\n",
    "        print('generating_full_prediction_list| df_prediction_raw_unkowns: {}'.format(df_prediction_raw_unkowns.shape))\n",
    "        \n",
    "        dict_uniprot_to_genes                          = self.__generate_uniprot_to_kinbase(self.__path_org_data, self.__gene_map_file)\n",
    "        dict_reactome_ALL_name, dict_reactome_HSA_name = self.__generate_reactome_to_name(self.__path_org_data, self.__reactome_map_file)\n",
    "        \n",
    "        list_prediction_with_name = []\n",
    "        for i in df_prediction_raw_unkowns.itertuples():\n",
    "            uniprot  = i[1]\n",
    "            reactome = i[2]\n",
    "            score    = i[3]\n",
    "            \n",
    "            protein_name =  dict_uniprot_to_genes[uniprot]\n",
    "            pathway_name = ''\n",
    "            try:\n",
    "                pathway_name = dict_reactome_HSA_name['R-HSA-' + reactome]\n",
    "            except:\n",
    "                pathway_name = dict_reactome_ALL_name[reactome]\n",
    "                \n",
    "            list_prediction_with_name.append([uniprot, protein_name, reactome, pathway_name, score])\n",
    "        \n",
    "        self.__df_prediction_with_name = pd.DataFrame(list_prediction_with_name\n",
    "                                                      , columns=['uniprot', 'protein_name', 'pathway_id', 'pathway_name', 'score'])\n",
    "        print('generating_full_prediction_list| df_prediction_with_names: {}'.format(self.__df_prediction_with_name.shape))\n",
    "        \n",
    "        self.__df_prediction_with_name.to_csv(self.__data_path + self.__result_path + 'prediction_with_names_' + self.__emb_file + '.csv', index=False)\n",
    "        print('generating_full_prediction_list| prediction with names saved.')\n",
    "        \n",
    "        \n",
    "        return self.__df_prediction_with_name\n",
    "    \n",
    "    def __wrtie_map_to_json(self, d, mapping_file_name):\n",
    "        for i in d:\n",
    "            d[i] = list(d[i])\n",
    "        self.__write_dict_to_json(d, self.__data_path + self.__result_path, mapping_file_name)\n",
    "        \n",
    "    \n",
    "    def __write_dict_to_json(self, d, filepath, filename):\n",
    "        with open(filepath + filename + '.txt', 'w') as file:\n",
    "            file.write(json.dumps(d))\n",
    "        \n",
    "    def __generate_reactome_to_name(self, path_org_data, file_name):\n",
    "        df_reactome = pd.read_csv(path_org_data + file_name, sep='\\t', names=['id', 'identifier', 'name', 'a', 'b', 'c'])\n",
    "        df_reactome_short = df_reactome[['identifier', 'a']].drop_duplicates(keep='first')\n",
    "        print('__generate_reactome_to_name| df_reactome_short: {}'.format(df_reactome_short.shape))\n",
    "        \n",
    "        dict_reactome_HSA_name = dict()\n",
    "        dict_reactome_ALL_name = dict()\n",
    "        for i in df_reactome_short.itertuples():\n",
    "            full_id = i[1]\n",
    "            name    = i[2]\n",
    "            if 'R-HSA' in full_id:\n",
    "                if full_id not in dict_reactome_HSA_name:\n",
    "                    dict_reactome_HSA_name[full_id] = set()\n",
    "                dict_reactome_HSA_name[full_id].add(name)\n",
    "                \n",
    "            short_id = full_id[full_id.rindex('-')+1:]\n",
    "            if short_id not in dict_reactome_ALL_name:\n",
    "                dict_reactome_ALL_name[short_id] = set()                \n",
    "            dict_reactome_ALL_name[short_id].add(name)\n",
    "            \n",
    "        print('__generate_reactome_to_name| dict_reactome_ALL_name: {}, dict_reactome_HSA_name: {}'.format(\n",
    "            len(dict_reactome_ALL_name), len(dict_reactome_HSA_name)))\n",
    "        mapping_file_name = 'reactometoid'\n",
    "        self.__wrtie_map_to_json(dict_reactome_ALL_name, mapping_file_name)\n",
    "        print('__generate_reactome_to_name| mapping file {}.txt is saved.'.format(mapping_file_name))\n",
    "        \n",
    "        return dict_reactome_ALL_name, dict_reactome_HSA_name\n",
    "        \n",
    "    def __generate_uniprot_to_kinbase(self, path_org_data, file_name):\n",
    "        df_uniprot_to_gene = pd.read_csv(path_org_data + file_name, sep='\\t')\n",
    "        print('__generate_uniprot_to_kinbase| df_uniprot_to_gene: {}'.format(df_uniprot_to_gene.shape))\n",
    "\n",
    "        unique_uniprot_from_uniprotIds = list(df_uniprot_to_gene['Entry'].unique())\n",
    "        print('__generate_uniprot_to_kinbase| unique_uniprot_from_uniprotIds : %d'%len(unique_uniprot_from_uniprotIds))\n",
    "\n",
    "        dict_uniprot_to_genes = dict()\n",
    "        for i in df_uniprot_to_gene.itertuples():\n",
    "            uniprot = i[1]\n",
    "            Gene = i[2]\n",
    "            if uniprot not in dict_uniprot_to_genes:\n",
    "                dict_uniprot_to_genes[uniprot] = set()\n",
    "            dict_uniprot_to_genes[uniprot].add(Gene)\n",
    "\n",
    "        print('__generate_uniprot_to_kinbase| dict_uniprot_to_genes: %d'% len(dict_uniprot_to_genes))\n",
    "        return dict_uniprot_to_genes\n",
    "        \n",
    "    \n",
    "    def __load_files(self):\n",
    "        mcols = self.__merge_cols\n",
    "        dtype = {mcols[0]:object, mcols[1]:object, mcols[2]:object, mcols[3]:int, mcols[4]:int, mcols[5]:int}\n",
    "        self.__df_merge     = pd.read_csv(self.__data_path + self.__df_merge_file ,dtype=dtype)\n",
    "        self.__df_nodes     = pd.read_csv(self.__data_path + self.__df_nodes_file)\n",
    "        self.__df_relations = pd.read_csv(self.__data_path + self.__df_relations_file)\n",
    "            \n",
    "        print('__load_files| df_merge: {}, df_nodes: {}, df_relations: {}'.format(self.__df_merge.shape, self.__df_nodes.shape\n",
    "                                                                                  , self.__df_relations.shape))\n",
    "        print()\n",
    "        \n",
    "    def __save_list_3cols(self, lst, file_name):\n",
    "        with open(self.__data_path + self.__result_path + 'predictions_raw_{}.csv'.format(file_name), 'w') as fp:\n",
    "            fp.write('\\n'.join('%s,%s,%s' % x for x in lst))\n",
    "    \n",
    "    def __get_df_pathway(self):\n",
    "        \n",
    "        self.__df_pathway = self.__df_merge[self.__df_merge['r'] == 'hasPathway']\n",
    "#         self.__df_pathway[\"t\"]= self.__df_pathway[\"t\"].astype(str)\n",
    "        print('__get_df_pathway| pathway: {}'.format(self.__df_pathway.shape[0]))\n",
    "        print()\n",
    "        \n",
    "    def __get_list_light_kinase(self):\n",
    "        df_light = pd.read_csv(self.__data_path + self.__light_file + '.csv')\n",
    "        df_light = df_light.rename(columns = {'uniprot':'id'})\n",
    "        self.__df_light = df_light\n",
    "        self.__list_light= list(self.__df_light['id'].unique())\n",
    "        print('__get_list_light_kinase| light list: {}'.format(len(self.__list_light)))\n",
    "        print()\n",
    "        \n",
    "    def __get_list_dark_kinase(self):\n",
    "        df_dark = pd.read_csv(self.__data_path + self.__dark_file +'.csv') \n",
    "        df_dark = df_dark.rename(columns = {'uniprot':'id'})\n",
    "        self.__df_dark = df_dark\n",
    "        self.__list_dark = list(self.__df_dark['id'].unique())\n",
    "        print('__get_list_dark_kinase| dark list: {}'.format(len(self.__list_dark)))\n",
    "        print()\n",
    "    \n",
    "    def __split_dark_N_light(self):\n",
    "        \n",
    "        mcols = self.__merge_cols\n",
    "        \n",
    "        self.__get_df_pathway()\n",
    "        self.__get_list_light_kinase()\n",
    "        self.__get_list_dark_kinase()\n",
    "        \n",
    "        df_dark_pathway = self.__df_pathway[self.__df_pathway[mcols[0]].isin(self.__list_dark)]\n",
    "        df_non_dark_pathway = self.__df_pathway[~self.__df_pathway[mcols[0]].isin(self.__list_dark)]\n",
    "\n",
    "        print('__split_dark_N_light| dark: {}, Non-Dark:{}'.format(df_dark_pathway.shape, df_non_dark_pathway.shape))\n",
    "        \n",
    "        self.__df_dark_pathway     = df_dark_pathway\n",
    "        self.__df_non_dark_pathway = df_non_dark_pathway\n",
    "        print()\n",
    "        \n",
    "    def __create_unique_list(self):\n",
    "        mcols = self.__merge_cols\n",
    "        \n",
    "        self.__list_unique_gene    = list(self.__df_pathway[mcols[3]].unique())\n",
    "        self.__list_unique_pathway = list(self.__df_pathway[mcols[4]].unique())\n",
    "        \n",
    "        print('__create_unique_list| gene: {}, pathway:{}'.format(len(self.__list_unique_gene), len(self.__list_unique_pathway)))\n",
    "        print()\n",
    "        \n",
    "    def __create_list_no(self):\n",
    "        \n",
    "        cols = self.__merge_cols\n",
    "        \n",
    "        self.__create_unique_list()\n",
    "        \n",
    "        self.__list_no  = list([])\n",
    "        \n",
    "        print('__create_list_no| df_non_dark_pathway.shape: {}, gene:{}, pathway: {}'.format(\n",
    "            self.__df_non_dark_pathway.shape[0], len(self.__list_unique_gene), len(self.__list_unique_pathway) ))\n",
    "        \n",
    "        check_tup = set(tuple(zip(self.__df_non_dark_pathway[cols[3]], self.__df_non_dark_pathway[cols[4]])))\n",
    "        \n",
    "        print('__create_list_no| check_tup: {}'.format(len(check_tup)))\n",
    "        \n",
    "        for i in range(self.__df_non_dark_pathway.shape[0]):\n",
    "            g = random.choice(self.__list_unique_gene)\n",
    "            p = random.choice(self.__list_unique_pathway )\n",
    "            if (str(p) in self.__emb_dict.keys() ) and (str(g) in self.__emb_dict.keys() ):  \n",
    "                if ((g,p) not in check_tup):\n",
    "                    self.__list_no.append([g, p, 0])   \n",
    "                    \n",
    "        \n",
    "        print('__create_list_no| list_no: {}'.format(len(self.__list_no)))\n",
    "        print()\n",
    "        \n",
    "    def __get_list_of_non_dark_embedding(self):\n",
    "        all_count = 0\n",
    "        self.__lst_emb_non_dark_pathway = list([])\n",
    "        for row in self.__df_non_dark_pathway.itertuples():\n",
    "            try:\n",
    "                self.__lst_emb_non_dark_pathway.append([row[0], self.__emb_dict[str(row[4])], self.__emb_dict[str(row[5])], 1])\n",
    "            except:\n",
    "                continue\n",
    "            all_count+=1\n",
    "        print('__get_list_of_non_dark_embedding| lst_emb_light_pathway: {} out of {}'.format(len(self.__lst_emb_non_dark_pathway), all_count))\n",
    "        print()\n",
    "\n",
    "        \n",
    "    def __get_training_examples(self):\n",
    "        self.__split_dark_N_light()\n",
    "        \n",
    "        self.__create_list_no()\n",
    "        \n",
    "        self.__get_list_of_non_dark_embedding()\n",
    "                       \n",
    "        self.__list_no_emb = [ [np.multiply(self.__emb_dict[str(i[0])],self.__emb_dict[str(i[1])]), i[2]]  for i in self.__list_no]\n",
    "\n",
    "        self.__list_yes_emb = [ [ np.multiply(i[1], i[2]), i[3]] for i in self.__lst_emb_non_dark_pathway]\n",
    "        \n",
    "        print('__get_training_examples| no combined emb: {}, yes combined emb: {}'.format(len(self.__list_no_emb), len(self.__list_yes_emb)))\n",
    "        \n",
    "        self.__list_train_emb = self.__list_yes_emb.copy()\n",
    "        self.__list_train_emb.extend(self.__list_no_emb)\n",
    "        random.shuffle(self.__list_train_emb)\n",
    "\n",
    "        lst_all = list()\n",
    "        for i in self.__list_train_emb:\n",
    "\n",
    "            lst = list([])\n",
    "            lst.extend(i[0])\n",
    "            lst.append(i[1])\n",
    "            \n",
    "            lst_all.append(lst)\n",
    "        self.__lst_all = lst_all\n",
    "        print('__get_training_examples| Training data generated.')\n",
    "        print()\n",
    "        \n",
    "    def __get_df_PPI(self):\n",
    "        \n",
    "        self.__df_PPI = self.__df_merge[self.__df_merge['r'] == 'interact']\n",
    "        print('__get_df_PPI| PPI: {}'.format(self.__df_PPI.shape[0]))\n",
    "        print()\n",
    "        \n",
    "    def __list_all_proteins_id(self):\n",
    "        cols = self.__merge_cols\n",
    "        self.__get_df_PPI()\n",
    "        lst_h  = list( self.__df_merge[cols[3]].unique() )\n",
    "        lst_ppi_t = list( self.__df_PPI[cols[4]].unique() )\n",
    "        self.__lst_proteins = list(set(lst_h + lst_ppi_t))\n",
    "        print('__list_all_proteins_id| lst_proteins: {}\\n'.format(len(self.__lst_proteins)))  \n",
    "        \n",
    "    def __label_all_node_embedding(self):\n",
    "        \n",
    "        emb_all_nodes = self.__emb_dict\n",
    "\n",
    "        lst_emb = []\n",
    "        lst_name = []\n",
    "        \n",
    "        self.__list_all_proteins_id() # we already found unique pathways\n",
    "        \n",
    "        set_all_proteins_id = set(self.__lst_proteins)\n",
    "        set_all_pathway_id = set(self.__list_unique_pathway)\n",
    "\n",
    "        for k in emb_all_nodes:  \n",
    "            lst = []\n",
    "            if (k != '/s>'):\n",
    "                label = 2\n",
    "                \n",
    "                if int(k) in set_all_proteins_id:\n",
    "                    label = 0\n",
    "                elif int(k) in set_all_pathway_id:\n",
    "                    label = 1\n",
    "\n",
    "                lst = [i for i in emb_all_nodes[k]]\n",
    "                lst.append(label)\n",
    "                lst_name.append(k)\n",
    "                lst_emb.append(lst)\n",
    "        print('__label_all_node_embedding| shape label embeddings: {} and {}'.format(len(lst_emb), len(lst_emb[0]) ))\n",
    "\n",
    "        self.__labeled_emb = lst_emb\n",
    "        \n",
    "    def __plot_pca(self, lst_emb, plot_name):\n",
    "        \n",
    "        df_emb2 = pd.DataFrame(lst_emb)\n",
    "        df_emb2_copy = df_emb2.copy()\n",
    "        df_emb2.drop(df_emb2.columns[[-2]], axis=1, inplace=True)\n",
    "        df_emb2 = df_emb2.dropna()\n",
    "        emb = df_emb2[:].values\n",
    "        #target\n",
    "        df_emb2_copy = df_emb2_copy.dropna()\n",
    "        df = df_emb2_copy.iloc[:,-1]\n",
    "\n",
    "\n",
    "\n",
    "        pca = PCA(n_components = 2)\n",
    "        principalComponents = pca.fit_transform(emb)\n",
    "        principalDf = pd.DataFrame(data = principalComponents\n",
    "                     , columns = ['principal component 1', 'principal component 2'])\n",
    "\n",
    "\n",
    "        finalDf = pd.concat([principalDf, df], axis = 1)\n",
    "        finalDf.columns = ['principal component 1', 'principal component 2', 'target']\n",
    "        # finalDf.head(2)\n",
    "\n",
    "        fig = plt.figure(figsize = (7,7))\n",
    "        ax = fig.add_subplot(1,1,1) \n",
    "        ax.set_xlabel('Principal Component 1', fontsize = 10)\n",
    "        ax.set_ylabel('Principal Component 2', fontsize = 10)\n",
    "        ax.set_title('2 component PCA', fontsize = 20)\n",
    "        targets = [0,1,2]\n",
    "        targets_name = ['Protein', 'Pathway', 'others']\n",
    "        colors = ['r', 'g','b']\n",
    "        for target, color in zip(targets,colors):\n",
    "            indicesToKeep = finalDf['target'] == int(target)\n",
    "            ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
    "                       , finalDf.loc[indicesToKeep, 'principal component 2']\n",
    "                       , c = color\n",
    "                       , s = 50)\n",
    "\n",
    "        ax.legend(targets_name)\n",
    "        ax.grid()\n",
    "        plt.savefig(self.__data_path + self.__result_path + plot_name)\n",
    "\n",
    "        # Protein  0,\n",
    "        # Other 1,\n",
    "        # relations 2\n",
    "        \n",
    "    def __precision_recall_curve(self, y_test, y_scores_lr , ver=''):\n",
    "\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, y_scores_lr)\n",
    "        closest_zero = np.argmin(np.abs(thresholds))\n",
    "        closest_zero_p = precision[closest_zero]\n",
    "        closest_zero_r = recall[closest_zero]\n",
    "\n",
    "        plt.figure(figsize=(6,6))\n",
    "        plt.xlim([0.0, 1.01])\n",
    "        plt.ylim([0.0, 1.01])\n",
    "        plt.plot(precision, recall, label='Precision-Recall Curve')\n",
    "\n",
    "        plt.xlabel('Precision', fontsize=16)\n",
    "        plt.ylabel('Recall', fontsize=16)\n",
    "        plt.axes().set_aspect('equal')\n",
    "        plt.show()\n",
    "        plt.savefig(self.__data_path + self.__result_path + 'precision_recall_curve' + ver)\n",
    "        \n",
    "        return precision, recall, thresholds\n",
    "    \n",
    "    def __AUC_ROC(self, y_test, y_score_lr, ver):\n",
    "        fpr_lr, tpr_lr, ths = roc_curve(y_test, y_score_lr)\n",
    "        self.thresholds = fpr_lr, tpr_lr, ths\n",
    "        roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "        \n",
    "        print('__AUC_ROC| roc_auc_lr: {}'.format(roc_auc_lr))\n",
    "\n",
    "        plt.figure()\n",
    "        plt.xlim([-0.01, 1.00])\n",
    "        plt.ylim([-0.01, 1.01])\n",
    "        plt.plot(fpr_lr, tpr_lr, lw=3, label='LogRegr ROC curve (area = {:0.2f})'.format(roc_auc_lr))\n",
    "        plt.xlabel('False Positive Rate', fontsize=16)\n",
    "        plt.ylabel('True Positive Rate', fontsize=16)\n",
    "        plt.title('ROC curve (1-of-10 digits classifier)', fontsize=16)\n",
    "        plt.legend(loc='lower right', fontsize=13)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\n",
    "        plt.axes().set_aspect('equal')\n",
    "        plt.show()\n",
    "        plt.savefig(self.__data_path + self.__result_path + 'roc_auc_lr' + ver)\n",
    "        \n",
    "    def __load_dark_hasPathway(self, file_name = 'df_dark_hasPathway.csv'):\n",
    "        return pd.read_csv(self.__data_path + file_name)\n",
    "    \n",
    "    def __get_dark_kinase_emb(self):\n",
    "        self.__lst_emb_dark_pathway = list([])\n",
    "        \n",
    "        df_dark_hasPathway = self.__load_dark_hasPathway()\n",
    "        \n",
    "        for row in df_dark_hasPathway.itertuples():\n",
    "            self.__lst_emb_dark_pathway.append([row[0], np.multiply(self.__emb_dict[str(row[4])], self.__emb_dict[str(row[5])]), 1])\n",
    "        print('__get_dark_kinase_emb| dark emb list from df_dark_hasPathway: {}'.format(len(self.__lst_emb_dark_pathway)))\n",
    "        \n",
    "    def __prepare_dark_emb(self):\n",
    "        \n",
    "        list_dark_yes_emb = [ [i[1], i[2]] for i in self.__lst_emb_dark_pathway]\n",
    "        self.__lst_dark_all = list()\n",
    "        for i in list_dark_yes_emb:\n",
    "\n",
    "            lst = list([])\n",
    "            lst.extend(i[0])\n",
    "            lst.append(i[1])\n",
    "            self.__lst_dark_all.append(lst)\n",
    "        \n",
    "        print('__prepare_dark_emb| list of dark prepared {}'.format(len(self.__lst_dark_all)))\n",
    "    \n",
    "    def __test(self, lst_to_test, sel_feat = False):        \n",
    "        if (sel_feat == True):\n",
    "            lst_dark_X = list([])\n",
    "            for item in lst_to_test:\n",
    "                l = list([])\n",
    "                for i in range(len(item)):\n",
    "                    if i in self.selected_feat_list:\n",
    "                        l.append(item[i])\n",
    "                lst_dark_X.append(l)\n",
    "                        \n",
    "                    \n",
    "#             lst_X = [[i[v]] for i in self.lst_all for v in range(len(i)) if v in self.selected_feat_list]\n",
    "        else:\n",
    "            lst_dark_X = [i[:-1] for i in lst_to_test]\n",
    "        lst_dark_y = [i[-1:] for i in lst_to_test]\n",
    "        \n",
    "        print('__test| {} and {}'.format(len(lst_dark_X), len(lst_dark_y)))\n",
    "        print('__test| {} and {}'.format(len(lst_dark_X[0]), len(lst_dark_y[0])))\n",
    "        \n",
    "            \n",
    "#         self.logreg.score(lst_dark_X, lst_dark_y)\n",
    "        lst_dark_pred = self.logreg.predict(lst_dark_X)\n",
    "        lst_dark_pred_prob = self.logreg.predict_proba(lst_dark_X)\n",
    "\n",
    "        confusionmatrix = confusion_matrix(lst_dark_y, lst_dark_pred)\n",
    "        print( confusionmatrix)\n",
    "        print(classification_report(lst_dark_y, lst_dark_pred))\n",
    "        \n",
    "        self.lst_dark_X = lst_dark_X\n",
    "        self.lst_dark_y = lst_dark_y\n",
    "        self.lst_dark_pred_prob = lst_dark_pred_prob\n",
    "        self.lst_dark_pred = lst_dark_pred\n",
    "        \n",
    "    def __get_df_dark_node(self):\n",
    "        \n",
    "        cols = self.__other_cols\n",
    "        self.__df_dark_node = self.__df_nodes[self.__df_nodes[cols[1]].isin(self.__list_dark)]\n",
    " \n",
    "        print('__get_df_dark_node| list_dark: {}, df_dark_node shape: {}, df_dark_node name unique: {} '.format(\n",
    "            len(self.__list_dark), self.__df_dark_node.shape[0],len(self.__df_dark_node['name'].unique()) ))\n",
    "    \n",
    "        \n",
    "    def __get_df_pathway_node(self):\n",
    "        \n",
    "        cols = self.__other_cols        \n",
    "        self.__df_pathway_node = self.__df_nodes[self.__df_nodes[cols[0]].isin(self.__list_unique_pathway)]\n",
    "               \n",
    "        print('__get_df_pathway_node| df_pathway_node: {}'.format(self.__df_pathway_node.shape[0]))    \n",
    "        \n",
    "        \n",
    "    def __get_df_human_pathway_node(self):\n",
    "                                  \n",
    "#         self.__df_pathway_nodes = self.__get_df_pathway_node()\n",
    "        self.__df_pathway_node\n",
    "        \n",
    "        dict_reactome_ALL_name, dict_reactome_HSA_name = self.__generate_reactome_to_name(self.__path_org_data, self.__reactome_map_file)\n",
    "        print('__get_df_human_pathway_node| dict_reactome_ALL_name: {}, dict_reactome_HSA_name: {}'.format(len(dict_reactome_ALL_name), len(dict_reactome_HSA_name)))\n",
    "        list_HSA      = list(dict_reactome_HSA_name.keys())\n",
    "        list_HSA_code = [i[i.rindex('-')+1:] for i in list_HSA]\n",
    "        print('__get_df_human_pathway_node| list_HSA: {}, list_HSA_code: {}'.format(len(list_HSA), len(list_HSA_code)))\n",
    "\n",
    "        self.__df_human_pathway_nodes = self.__df_pathway_node[self.__df_pathway_node['name'].isin(list_HSA_code)]\n",
    "        print('__filter_human_pathways| __df_pathway_node: {}'.format(self.__df_human_pathway_nodes.shape))\n",
    "        \n",
    "            \n",
    "    def get__df_pathway(self):\n",
    "        return self.__df_pathway\n",
    "    \n",
    "    def get__df_merge(self):\n",
    "        return self.__df_merge\n",
    "    \n",
    "    def get__df_dark_pathway(self):\n",
    "        return self.__df_dark_pathway\n",
    "    \n",
    "    def get__emb_dict(self):\n",
    "        return self.__emb_dict\n",
    "    \n",
    "    def get__list_unique_pathway(self):\n",
    "        return self.__list_unique_pathway\n",
    "    \n",
    "    def get__df_nodes(self):\n",
    "        return self.__df_nodes\n",
    "    \n",
    "    def get__df_merge(self):\n",
    "        return self.__df_merge\n",
    "    \n",
    "    def get_the_model(self):\n",
    "        return self.logreg\n",
    "    \n",
    "    def get__dict_ppid_pred(self):\n",
    "        return self.__dict_ppid_pred\n",
    "    \n",
    "    def get__dict_ppid_emb(self):\n",
    "        return self.__dict_ppid_emb\n",
    "\n",
    "    \n",
    "    def __read_embeddings(self):\n",
    "\n",
    "        filepath = self.__data_path + self.__emb_file + '.txt'\n",
    "        self.__emb_dict = dict([])\n",
    "        chk_list = []\n",
    "        with open(filepath) as fp:\n",
    "            line = fp.readline().split(' ')\n",
    "            print('__read_embeddings| row: {}, col:{}'.format(line[0], line[1]))\n",
    "            line = fp.readline()\n",
    "            cnt = 0\n",
    "            while line:\n",
    "                \n",
    "                \n",
    "                strLine = line.split(' ')\n",
    "                entity_id = strLine[0][1:]\n",
    "                vector = strLine[1:-1]                \n",
    "                vec = [float(i) for i in vector]\n",
    "                self.__emb_dict[entity_id] = vec\n",
    "                cnt += 1\n",
    "                chk_list.append(entity_id)\n",
    "                line = fp.readline()\n",
    "        \n",
    "        print('__read_embeddings| size of emb: {}, count line: {}, list chk: {}'.format(len(self.__emb_dict), cnt, len(chk_list) ))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = LinkPrediction('data/', 'df_merge_cc_train.csv', 'df_nodes_cc.csv', 'df_relations.csv'\n",
    "                    , 'emb-while_loop_no_raise_all_pro', 'light_kinase'\n",
    "                    , 'dark_kinase','df_merge_cc_test.csv', 'results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.draw_pca_of_all_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lp.train_model_cross_validation('newton-cg', 'ovr', cv = 10, ver = '_100_40_cv2_biased-walk-while-loop-raise-all-pro.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lp.test_dark_kinase(False)\n",
    "lp.evaluate_test_set(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp.test_dark_kinase(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst_sig = lp.statistical_significance_for_each_prediction(path_org_data, gene_map_file, reactome_map_file, \n",
    "#                                                 n_iter = 6, p = 0.5, alternative='two-sided', pvalue = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr_sig = np.array(lst_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(lst_sig).count(1), len(lst_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d_ppic_pred = lp.get__dict_ppid_pred()\n",
    "# print('d_ppic_pred: {}'.format(len(d_ppic_pred)))\n",
    "\n",
    "# set_count = set()\n",
    "# for k in d_ppic_pred:\n",
    "#     if 1 in d_ppic_pred[k]:\n",
    "#         count_1 = d_ppic_pred[k].count(1)\n",
    "#         set_count.add(count_1)\n",
    "# set_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_org_data, gene_map_file, reactome_map_file = 'data/original/','Protein_Kinase_List_v3.txt','UniProt2Reactome_All_Levels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_prediction, list_prediction_all = lp.predict_dark_pathway('df_merge_cc.csv',path_org_data, gene_map_file, reactome_map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_prediction.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction_with_name = lp.generating_full_prediction_list()\n",
    "\n",
    "# Protein_Kinase_List_v3.txt or uniprotIds.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction_with_name.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ths = lp.thresholds\n",
    "fpr_lr, tpr_lr, th = ths\n",
    "# https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = list(zip(fpr_lr,tpr_lr))\n",
    "ls[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.arange(len(tpr_lr)) \n",
    "roc = pd.DataFrame({'tf' : pd.Series(tpr_lr-(1-fpr_lr), index=i), 'threshold' : pd.Series(th, index=i)})\n",
    "roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "\n",
    "list(roc_t['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The rewire process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpr = LinkPrediction('data/', 'df_merge_rew_train-NoSplit_1.csv', 'df_nodes_cc.csv', 'df_relations.csv'\n",
    "                    , 'emb-pro-x-pro-path-100-40-graphpattern2vec-biased-v2-rewire-merge_rew_train_NoSplit_1', 'light_kinase'\n",
    "                    , 'dark_kinase','df_merge_rew_test-NoSplit_1.csv', 'results/rewire1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpr.draw_pca_of_all_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpr.train_model_cross_validation('newton-cg', 'ovr', cv = 2, ver = '_100_40_cv2_biased-v2-rewire-NoSplit_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lp.test_dark_kinase(False)\n",
    "lpr.evaluate_test_set(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpr.test_dark_kinase(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_org_data, gene_map_file, reactome_map_file = 'data/original/','Protein_Kinase_List_v3.txt','UniProt2Reactome_All_Levels.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_prediction, list_prediction_all = lpr.predict_dark_pathway('df_merge_cc.csv',path_org_data, gene_map_file, reactome_map_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The protien in Kannan interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_Q96C45 = df_prediction_with_name[df_prediction_with_name['uniprot'] == 'Q96C45']\n",
    "print('df_pred_Q96C45: {}'.format(df_pred_Q96C45.shape))\n",
    "\n",
    "df_pred_Q96C45_top80prec = df_pred_Q96C45[df_pred_Q96C45['score'] > 0.80]\n",
    "print('df_pred_Q96C45_top80prec: {}'.format(df_pred_Q96C45_top80prec.shape))\n",
    "\n",
    "df_pred_Q96C45_top80prec.to_csv('data/df_pred_Q96C45_top80prec.csv')\n",
    "\n",
    "df_pred_Q96C45_top80prec.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check \n",
    "# df_pred_Q96C45[df_pred_Q96C45['pathway_id'] == '5610783']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tudor_list = ['O15021', 'O43930', 'P0C264', 'P51957', 'Q00532', 'Q504Y2', 'Q56UN5', 'Q6A1A2', 'Q6P3R8', 'Q6ZMQ8', 'Q6ZN16', \n",
    "             'Q86YV5', 'Q8IVW4', 'Q8IWB6', 'Q8IZE3', 'Q8N2I9', 'Q8N568', 'Q8NE28', 'Q8TDR2', 'Q96C45', 'Q96LW2', 'Q96PN8',\n",
    "             'Q96QS6', 'Q9C098', 'Q9NSY0', 'Q9NY57']\n",
    "print('tudor_list: {}'.format(len(tudor_list)))\n",
    "\n",
    "df_tudor = df_prediction_with_name[df_prediction_with_name['uniprot'].isin(tudor_list)]\n",
    "print('df_tudor: {}'.format(df_tudor.shape))\n",
    "\n",
    "df_tudor.to_csv('data/tudor_list_prediction_by_FixedII.csv')\n",
    "print('df_tudor full is saved.')\n",
    "\n",
    "df_tudor_g = df_tudor.groupby('uniprot')['pathway_name'].count().reset_index(name = 'count')\n",
    "print('df_tudor_g: {}'.format(df_tudor_g.shape))\n",
    "\n",
    "df_tudor_top80pred = df_tudor[df_tudor['score'] > 0.80]\n",
    "print('df_tudor_top80pred: {}'.format(df_tudor_top80pred.shape))\n",
    "\n",
    "df_tudor_top80pred.to_csv('data/tudor_list_prediction_by_FixedII_biased-v2-score80.csv')\n",
    "print('df_tudor_top80pred full is saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check common predictions\n",
    "df_18 = df_prediction_with_name[df_prediction_with_name['uniprot'] == 'Q07002']\n",
    "df_19 = df_prediction_with_name[df_prediction_with_name['uniprot'] == 'Q9BWU1']\n",
    "\n",
    "list_18 = (list(df_18['pathway_name']))\n",
    "list_19 = (list(df_19['pathway_name']))\n",
    "\n",
    "# df_18.shape, df_19.shape\n",
    "common_18_19 = [i for i in list_18 if i in list_19]\n",
    "len(common_18_19)\n",
    "\n",
    "# sample: missing protein id form emb\n",
    "\n",
    "a = [100518, 51760, 168228,32097, 423]\n",
    "df_n = lp.get__df_nodes()\n",
    "df_n[df_n['id'].isin(a)]\n",
    "\n",
    "# 423,    Q9Y463, DYRK1B, Dual specificity tyrosine-phosphorylation-regulated kinase 1B [dark]\n",
    "# 32097,  O15021, MAST4,  Microtubule-associated serine/threonine-protein kinase 4      [dark]\n",
    "# 51760,  Q6P0Q8, MAST2, Microtubule-associated serine/threonine-protein kinase 2       [dark]\n",
    "# 100518, Q9BXA6, TSSK6, Testis-specific serine/threonine-protein kinase 6              [dark]\n",
    "# 168228, Q96QS6, PSKH2, Serine/threonine-protein kinase H2                             [dark]\n",
    "\n",
    "# we have 156 dark kinase node, 39 of them are missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check how many predictions are for Human Pathway vs Non Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions over R-HSA \n",
    "def hist_array(a, title):\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    print('min: {}, max: {}'.format(np.min(a), np.max(a)))\n",
    "    _ = plt.hist(a, bins='auto')  # arguments are passed to np.histogram\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def generate_reactome_to_name( path_org_data, file_name):\n",
    "        df_reactome = pd.read_csv(path_org_data + file_name, sep='\\t', names=['id', 'identifier', 'name', 'a', 'b', 'c'])\n",
    "        df_reactome_short = df_reactome[['identifier', 'a']].drop_duplicates(keep='first')\n",
    "        print('__generate_reactome_to_name| df_reactome_short: {}'.format(df_reactome_short.shape))\n",
    "        \n",
    "        dict_reactome_HSA_name = dict()\n",
    "        dict_reactome_ALL_name = dict()\n",
    "        for i in df_reactome_short.itertuples():\n",
    "            full_id = i[1]\n",
    "            name    = i[2]\n",
    "            if 'R-HSA' in full_id:\n",
    "                if full_id not in dict_reactome_HSA_name:\n",
    "                    dict_reactome_HSA_name[full_id] = set()\n",
    "                dict_reactome_HSA_name[full_id].add(name)\n",
    "            short_id = full_id[full_id.rindex('-')+1:]\n",
    "            if short_id not in dict_reactome_ALL_name:\n",
    "                dict_reactome_ALL_name[short_id] = set()                \n",
    "            dict_reactome_ALL_name[short_id].add(name)\n",
    "            \n",
    "        print('__generate_reactome_to_name| dict_reactome_ALL_name: {}, dict_reactome_HSA_name: {}'.format(\n",
    "            len(dict_reactome_ALL_name), len(dict_reactome_HSA_name)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        return dict_reactome_ALL_name, dict_reactome_HSA_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, d  = generate_reactome_to_name('data/original/', 'UniProt2Reactome_All_Levels.txt')\n",
    "k = [i[i.rindex('-')+1:] for i in d.keys()]\n",
    "df_non_HSA = df_prediction_with_name[~df_prediction_with_name['pathway_id'].isin(k)]\n",
    "df_HSA = df_prediction_with_name[df_prediction_with_name['pathway_id'].isin(k)]\n",
    "print('df_HSA: {}, df_non_HSA: {}'.format(df_HSA.shape, df_non_HSA.shape))\n",
    "arr_n_hsa = np.array(df_non_HSA['score'])\n",
    "arr_hsa = np.array(df_HSA['score'])\n",
    "arr_n_hsa[:2], arr_hsa[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_array(arr_hsa, 'Human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist_array(arr_n_hsa, 'None Human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the trained model for other pairs of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = lp.get__df_merge()\n",
    "df_n = lp.get__df_nodes()\n",
    "em = lp.get__emb_dict()\n",
    "uniq_pathway = lp.get__list_unique_pathway()\n",
    "model = lp.get_the_model()\n",
    "# ['hasPathway', 'hasFunctionalDomain', 'interact', 'hasPTM', 'hasCellularComponent', 'hasMolecularFunction', 'hasBiologicalProcess']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_by_relation(df_m,df_n, r, emb, h_list, model):\n",
    "    print('h_list: {}'.format(len(h_list)))\n",
    "    \n",
    "    list_nodeType_id = list(df_m.loc[df_m['r'] == r, 't_id'].unique())\n",
    "    print('list_nodeType_id: {}'.format(len(list_nodeType_id)))\n",
    "    \n",
    "    df_nodes_type = df_n[df_n['id'].isin(list_nodeType_id)]\n",
    "    print('df_nodes_type: {}'.format(df_nodes_type.shape))\n",
    "    \n",
    "    print(list_nodeType_id[:3])\n",
    "    \n",
    "    emb_node_type = { i:emb[str(i)] for i in list_nodeType_id if str(i) in emb.keys() }\n",
    "    print('emb_node_type: {}'.format(len(emb_node_type)))\n",
    "    \n",
    "    emb_h_list = { i:emb[str(i)] for i in h_list if str(i) in emb.keys() }\n",
    "    print('emb_h_list: {}'.format(len(emb_h_list)))\n",
    "    \n",
    "    predicted_true_count = 0\n",
    "    predicted_false_count = 0\n",
    "    result = list()\n",
    "    \n",
    "    lst_h_keys    = list(emb_h_list.keys())\n",
    "    lst_node_keys = list(emb_node_type.keys())\n",
    "    \n",
    "    for i in tqdm(range(2000)):\n",
    "        h = random.choice(lst_h_keys)\n",
    "        t = random.choice(lst_node_keys)\n",
    "        \n",
    "        e = np.multiply(emb_h_list[h], emb_node_type[t])\n",
    "        \n",
    "        prediction = model.predict([e])\n",
    "        prob       = model.predict_proba([e])\n",
    "\n",
    "        if (prediction ==1):\n",
    "            result.append([prob])\n",
    "            predicted_true_count += 1\n",
    "        else:\n",
    "            predicted_false_count+=1\n",
    "            \n",
    "    return result\n",
    "    \n",
    "lst = get_nodes_by_relation(df_m, df_n, 'hasMolecularFunction' ,em, uniq_pathway, model)\n",
    "len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.read_csv('data/prediction_with_names_emb-pro-x-pro-path-100-40-graphpattern2vec-HL-Path-Biased-fixedcsv')\n",
    "df_p.loc[df_p['uniprot'] == 'Q96C45',:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_m = pd.read_csv('data/df_merge_cc.csv')\n",
    "df_m.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_m.loc[df_m['h'] == 'Q96C45','r'].unique() #'interact', 'hasMolecularFunction', 'hasBiologicalProcess'\n",
    "# df_m[(df_m['h'] == 'Q96C45') & (df_m['r'] == 'hasPathway')]\n",
    "df_Q9  = df_m[(df_m['h'] == 'Q96C45') & (df_m['r'] == 'interact')]\n",
    "lst_Q9 = list(df_Q9['t'].unique())\n",
    "len(lst_Q9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Q9_ppi_pathways = df_m.loc[(df_m['h'].isin(lst_Q9) )]\n",
    "lst_Q9_ppi_pathways = list(df_Q9_ppi_pathways['t_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lst_Q9_ppi_pathways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n = lp.get__df_nodes_cc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcols = ['h','t','r','h_id','t_id', 'r_id']\n",
    "dtype = {mcols[0]:object, mcols[1]:object, mcols[2]:object, mcols[3]:int, mcols[4]:int, mcols[5]:int}\n",
    "df_all_merge = pd.read_csv('data/df_merge_cc.csv' ,dtype=dtype)\n",
    "print('predict_dark_pathway| df_all_merge: {}'.format(df_all_merge.shape))\n",
    "\n",
    "df_p = df_all_merge[df_all_merge[mcols[2]] == 'hasPathway']\n",
    "print('predict_dark_pathway| df_p: {}'.format(df_p.shape))\n",
    "\n",
    "\n",
    "list_unique_pathway_str = list(df_p[mcols[1]].unique())\n",
    "# list_unique_pathway = [ for i in list_unique_pathway_str]\n",
    "\n",
    "print('predict_dark_pathway| list_unique_pathway: {}'.format(len(list_unique_pathway)))\n",
    "\n",
    "\n",
    "df_pathway_node = df_n[df_n['name'].isin(list_unique_pathway_str)]\n",
    "df_pathway_node.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_nodes = set(df_n['name'].unique())\n",
    "len(set_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for i in df_p.itertuples():\n",
    "    t = i[2]\n",
    "    if t in set_nodes:\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
