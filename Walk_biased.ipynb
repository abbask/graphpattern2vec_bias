{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WalkBiased(object):\n",
    "    \n",
    "    def __init__(self, data_path, merge_file, node_file, relation_file, node_to_class_file, d_bias,  all_proteins = False):\n",
    "        self.zero_dist_len = 0\n",
    "        self.__data_path = data_path\n",
    "        \n",
    "        self.__merge_file = merge_file\n",
    "        self.__node_file = node_file\n",
    "        self.__relation_file = relation_file   \n",
    "        self.__node_to_class_file = node_to_class_file\n",
    "        self.__d_bias = d_bias\n",
    "        \n",
    "        self.__edge_col_names  = ['h','t','r'] \n",
    "        self.__merge_col_names = ['h','t','r', 'h_id', 't_id', 'r_id']\n",
    "        self.__other_col_names = ['id', 'name']\n",
    "        self.__merge_col_id =    ['h_id', 't_id', 'r_id']\n",
    "        \n",
    "        self.__all_proteins = all_proteins\n",
    "        \n",
    "        self.__read_files()\n",
    "        self.__get_node_groupby()\n",
    "        self.__load_node_to_class()\n",
    "        self.__set_biased()\n",
    "        self.__prepare_dicts()\n",
    "        \n",
    "        self.__validate_all_dicts()\n",
    "        \n",
    "    def __load_node_to_class(self):\n",
    "        with open(data_path + self.__node_to_class_file + '.pkl', 'rb') as f:\n",
    "            self.__d_node_to_class = pickle.load(f)\n",
    "        print('__load_node_to_class| __d_node_to_class: {}'.format(len(self.__d_node_to_class)))\n",
    "    \n",
    "    def __sets_to_lists(self, d, biased = False):\n",
    "        r = dict()\n",
    "        c = 0\n",
    "        for p in d:\n",
    "            \n",
    "            try:\n",
    "                lst = list(d[p])\n",
    "                if biased == False:\n",
    "                    \n",
    "                    prob = self.__get_dist_point(lst)\n",
    "                else:\n",
    "                    prob = self.__get_dist(lst)\n",
    "                r[p] = lst\n",
    "                r[p].append(prob)\n",
    "            except:\n",
    "                c+=1\n",
    "                continue\n",
    "        print('{} out of {}'.format(c, len(d)))\n",
    "        return r\n",
    "        \n",
    "    def __change_dict_value_type(self):\n",
    "        print('__change_dict_value_type| started.')\n",
    "        self.__pro_other  = self.__sets_to_lists(self.__pro_other, True)\n",
    "        self.__other_pro  = self.__sets_to_lists(self.__other_pro)\n",
    "        self.__pro_other2 = self.__sets_to_lists(self.__pro_other2, True)\n",
    "        self.__other_pro2 = self.__sets_to_lists(self.__other_pro2)\n",
    "        self.__pro_path   = self.__sets_to_lists(self.__pro_path)\n",
    "        self.__path_pro   = self.__sets_to_lists(self.__path_pro)\n",
    "        print('__change_dict_value_type| done.')\n",
    "        print()\n",
    "        \n",
    "    def __prepare_dicts(self):\n",
    "        self.__prepare_dict_pro_path()\n",
    "        self.__prepare_dict_pro_x()\n",
    "        self.__change_dict_value_type()\n",
    "        print('__prepare_dicts| self.zero_dist_len: {}'.format(self.zero_dist_len))\n",
    "        \n",
    "    def __get_data_ids_by_relation(self, list_relations, isin = True):\n",
    "        if isin == False:\n",
    "                return self.__df_merge[~self.__df_merge['r'].isin(list_relations)][self.__merge_col_id]\n",
    "        return self.__df_merge[self.__df_merge['r'].isin(list_relations)][self.__merge_col_id]\n",
    "     \n",
    "    def __prepare_dict_pro_x(self):\n",
    "        mcols = self.__merge_col_id\n",
    "        \n",
    "        df_others_short = self.__get_data_ids_by_relation(['hasPathway'], isin = False)\n",
    "        print('__prepare_dict_pro_x| df_other: {}'.format(df_others_short.shape))\n",
    "        \n",
    "        print('__prepare_dict_pro_x| processing the other to protein (other_pro) ...')\n",
    "        \n",
    "        if self.__all_proteins == False:\n",
    "            lst_pro_path_Proteins = list(self.__pro_path.keys())\n",
    "            print('__prepare_dict_pro_x| lst_pro_path_Proteins: {}'.format(len(lst_pro_path_Proteins)))\n",
    "        else:\n",
    "            print('__prepare_dict_pro_x| ALL PROTEINS MODE:')\n",
    "            lst_pro_path_Proteins = list(self.__df_merge['h_id'].unique())\n",
    "            print('__prepare_dict_pro_x| list all proteins: {}'.format(len(lst_pro_path_Proteins)))\n",
    "    \n",
    "        df_others_short_selected_proteins = df_others_short[df_others_short[mcols[0]].isin(lst_pro_path_Proteins)]\n",
    "        print('__prepare_dict_pro_x| df_others_short_selected: {}'.format(df_others_short_selected_proteins.shape))\n",
    "        \n",
    "        other_pro = dict()\n",
    "\n",
    "        for tup in df_others_short_selected_proteins.itertuples():\n",
    "            if tup[2] not in other_pro:\n",
    "                other_pro[tup[2]] = set()\n",
    "            other_pro[tup[2]].add((tup[1],tup[3]))\n",
    "\n",
    "        self.__other_pro = other_pro\n",
    "        print('__prepare_dict_pro_x| other_pro: {}'.format(len(other_pro))) \n",
    "        \n",
    "        print('__prepare_dict_pro_x| processing the protein to other (pro_other) ...')\n",
    "        \n",
    "        lst_other_pro_Others = list(other_pro.keys())\n",
    "        print('__prepare_dict_pro_x| lst_other_pro_Others: {}'.format(len(lst_other_pro_Others)))\n",
    "        \n",
    "        df_others_short_selected_others = df_others_short[df_others_short[mcols[1]].isin(lst_other_pro_Others)]\n",
    "        print('__prepare_dict_pro_x| df_others_short_selected_others: {}'.format(df_others_short_selected_others.shape))\n",
    "        \n",
    "        pro_other = dict()\n",
    "\n",
    "        for tup in df_others_short_selected_others.itertuples():\n",
    "\n",
    "            if tup[1] not in pro_other:\n",
    "                pro_other[tup[1]] = set()\n",
    "            pro_other[tup[1]].add((tup[2],tup[3]))\n",
    "        \n",
    "        self.__pro_other = pro_other\n",
    "        print('__prepare_dict_pro_x| pro_other: {}'.format(len(pro_other))) \n",
    "        \n",
    "        print('__prepare_dict_pro_x| The Final dict: pro_other: {}, other_pro: {}'.format(len(pro_other), len(other_pro))) \n",
    "        \n",
    "        print('__prepare_dict_pro_x| processing the protein to other 2 (pro_other2) ...')\n",
    "        \n",
    "        lst_path_pro_Proteins = [j for i in self.__path_pro for j in self.__path_pro[i]]\n",
    "        print('__prepare_dict_pro_x| lst_path_pro_Proteins: {}'.format(len(lst_path_pro_Proteins)))\n",
    "        \n",
    "        df_shorts_short_seleected_proteins2 = df_others_short[df_others_short[mcols[0]].isin(lst_path_pro_Proteins)]\n",
    "        print('__prepare_dict_pro_x| df_shorts_short_seleected_proteins2: {}'.format(df_shorts_short_seleected_proteins2.shape))\n",
    "        \n",
    "        pro_other2 = dict()\n",
    "\n",
    "        for tup in df_shorts_short_seleected_proteins2.itertuples():\n",
    "\n",
    "            if tup[1] not in pro_other2:\n",
    "                pro_other2[tup[1]] = set()\n",
    "            pro_other2[tup[1]].add((tup[2],tup[3]))\n",
    "\n",
    "        self.__pro_other2 = pro_other2\n",
    "        print('__prepare_dict_pro_x| pro_other2: {}'.format(len(pro_other2))) \n",
    "        \n",
    "        print('__prepare_dict_pro_x| processing the other to protein 2 (other_pro2) ...')\n",
    "        \n",
    "        lst_pro_other2_Others = [j[0] for i in pro_other2 for j in pro_other2[i]]\n",
    "        print('__prepare_dict_pro_x| lst_pro_other2_Others: {}'.format(len(lst_pro_other2_Others)))\n",
    "        \n",
    "        df_shorts_short_seleected_others2 = df_others_short[df_others_short['t_id'].isin(lst_pro_other2_Others)]\n",
    "        print('__prepare_dict_pro_x| df_shorts_short_seleected_others2: {}'.format(df_shorts_short_seleected_others2.shape))\n",
    "        \n",
    "        other_pro2 = dict()\n",
    "        for tup in df_shorts_short_seleected_others2.itertuples():\n",
    "\n",
    "            if tup[2] not in other_pro2:\n",
    "                other_pro2[tup[2]] = set()\n",
    "            other_pro2[tup[2]].add((tup[1],tup[3]))\n",
    "\n",
    "        self.__other_pro2 = other_pro2\n",
    "        print('__prepare_dict_pro_x| other_pro2: {}'.format(len(other_pro2))) \n",
    "        \n",
    "        print('__prepare_dict_pro_x| The Final dict: pro_other2: {}, other_pro2: {}'.format(len(pro_other2), len(other_pro2))) \n",
    "\n",
    "        print()\n",
    "    \n",
    "        \n",
    "    def __prepare_dict_pro_path(self):\n",
    "    \n",
    "        df_hasPathway = self.__get_data_ids_by_relation(['hasPathway'])\n",
    "        print('__prepare_dict_pro_path| df_hasPathway: {}'.format(df_hasPathway.shape))\n",
    "        \n",
    "        pro_path = dict()\n",
    "        path_pro = dict()\n",
    "\n",
    "        for tup in df_hasPathway.itertuples():\n",
    "            if tup[1] not in pro_path:\n",
    "                pro_path[tup[1]] = set()\n",
    "            pro_path[tup[1]].add(tup[2])\n",
    "            if tup[2] not in path_pro:\n",
    "                path_pro[tup[2]] = set()\n",
    "            path_pro[tup[2]].add(tup[1])\n",
    "\n",
    "        self.__pro_path = pro_path\n",
    "        self.__path_pro = path_pro\n",
    "        \n",
    "        print('__prepare_dict_pro_path| pro_path: {}, path_pro: {}'.format(len(pro_path), len(path_pro)))\n",
    "        print()\n",
    "    \n",
    "    def __read_files(self):\n",
    "        mcols = self.__merge_col_names\n",
    "        \n",
    "        self.__df_merge = pd.read_csv(self.__data_path + self.__merge_file + '.csv'\n",
    "                                      , dtype={mcols[0]: object, mcols[1]: object, mcols[2]:object, mcols[3]:int\n",
    "                                               , mcols[4]:int, mcols[5]:int})\n",
    "        print('__read_processed_files| df_merge: {}'.format(self.__df_merge.shape))\n",
    "        \n",
    "        self.__df_nodes = pd.read_csv(self.__data_path + self.__node_file + '.csv')\n",
    "        print('__read_processed_files| df_nodes: {}'.format(self.__df_nodes.shape))\n",
    "        \n",
    "        self.__df_relations = pd.read_csv(self.__data_path + self.__relation_file + '.csv')\n",
    "        print('__read_processed_files| df_relations: {}'.format(self.__df_relations.shape))\n",
    "        \n",
    "        print()  \n",
    "        \n",
    "    def do_biased_walks(self, numwalks, walk_len, desc = '-rem-HL', seed = 5):\n",
    "        walk_file_path = 'walks-pro-x-pro-path-{}-{}-graphpattern2vec{}'.format(numwalks, walk_len, desc)\n",
    "        \n",
    "        # v for protein, a for pathway, f for others\n",
    "\n",
    "        error_count = 0\n",
    "        dc = 0\n",
    "        \n",
    "        pro_other  = self.__pro_other\n",
    "        other_pro  = self.__other_pro \n",
    "        pro_other2 = self.__pro_other2 \n",
    "        other_pro2 = self.__other_pro2\n",
    "        pro_path   = self.__pro_path\n",
    "        path_pro   = self.__path_pro\n",
    "        \n",
    "        random.seed(seed)\n",
    "        outfile = open(data_path + walk_file_path, 'w')\n",
    "        \n",
    "        count_ppi, count_non_ppi = 0, 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        for pr in tqdm(pro_other):\n",
    "\n",
    "            \n",
    "            for j in range(numwalks ): #wnum walks\n",
    "                start = pr\n",
    "                outline = ' v' + str(start)\n",
    "                try:\n",
    "                    i = 0\n",
    "                    while i < walk_len:    \n",
    "                        break_save = False\n",
    "                        \n",
    "                        others0 = pro_other[start]                        \n",
    "                        \n",
    "                        other0 = random.choices(others0[:-1], others0[-1])[0]\n",
    "                        \n",
    "                        num_try = len(others0) * 10\n",
    "                        \n",
    "                        while(other0[1] == 2 and num_try !=0):\n",
    "                            outline += ' v' + str(other0[0] )\n",
    "                            i += 1\n",
    "                            if i >= walk_len:\n",
    "                                break_save = True\n",
    "                                break;\n",
    "                            \n",
    "                            \n",
    "                            if other0[0] in pro_other.keys():\n",
    "                                \n",
    "                                lst_non_ppi = [i for i in list(pro_other[other0[0]][:-1]) if i[1] != 2]     \n",
    "                                if len(lst_non_ppi) == 0 :\n",
    "                                    other0 = random.choices(others0[:-1], others0[-1])[0]\n",
    "                                else:                                    \n",
    "                                    prob = self.__get_dist(lst_non_ppi)\n",
    "\n",
    "                                    other0 = random.choices(lst_non_ppi, prob)[0]\n",
    "                                    break\n",
    "                            else:\n",
    "                                other0 = random.choices(others0[:-1], others0[-1])[0]\n",
    "\n",
    "                            num_try-=1   \n",
    "                            \n",
    "                         \n",
    "                        if (num_try == 0):\n",
    "                            break_save = False\n",
    "                            break;\n",
    "                        \n",
    "                        if (break_save == True):\n",
    "                            break_save = False\n",
    "                            break;\n",
    "                            \n",
    "                            \n",
    "                        outline += ' f' + str(other0[0] )\n",
    "                        i += 1\n",
    "                        if i >= walk_len:\n",
    "                            break;\n",
    "                            \n",
    "                        \n",
    "                        pros = other_pro[other0[0]]\n",
    "                        start = random.choices(pros[:-1], pros[-1])[0][0]\n",
    "                        outline += ' v' + str(start )\n",
    "                        i += 1\n",
    "                        if i >= walk_len:\n",
    "                            break;\n",
    "                        \n",
    "                        paths0 = pro_path[start]\n",
    "                        path0 = random.choices(paths0[:-1], paths0[-1])[0]\n",
    "                        outline += ' a' + str(path0 )\n",
    "                        i += 1\n",
    "                        if i >= walk_len:\n",
    "                            break;\n",
    "\n",
    "\n",
    "                        pros1 = path_pro[path0]\n",
    "                        pro1 = random.choices(pros1[:-1], pros1[-1])\n",
    "                        outline += ' v' + str(pro1[0] )\n",
    "                        i += 1\n",
    "                        if i >= walk_len:\n",
    "                            break;\n",
    "                            \n",
    "                        others1 = pro_other2[pro1[0]]\n",
    "                        other1 = random.choices(others1[:-1], others1[-1])[0]\n",
    "\n",
    "                        num_try = len(others1)\n",
    "                        \n",
    "                        while(other1[1] == 2 and num_try !=0):\n",
    "                            outline += ' v' + str(other1[0] )\n",
    "                            i += 1\n",
    "                            if i >= walk_len:                            \n",
    "                                break_save = True\n",
    "                                break;\n",
    "                            \n",
    "                            if other1[0] in pro_other2.keys():\n",
    "                                lst_non_ppi = [i for i in list(pro_other2[other1[0]][:-1]) if i[1] != 2]     \n",
    "                                if len(lst_non_ppi) == 0 :\n",
    "                                    other1 = random.choices(others1[:-1], others1[-1])[0]\n",
    "                                else:\n",
    "                                    \n",
    "                                    prob = self.__get_dist(lst_non_ppi)\n",
    "                                    other1 = random.choices(lst_non_ppi, prob)[0]\n",
    "                                    break\n",
    "                                                                        \n",
    "                            else:\n",
    "                                \n",
    "                                other1 = random.choices(others1[:-1], others1[-1])[0]\n",
    "\n",
    "                            num_try-=1   \n",
    "                        if (num_try == 0):\n",
    "                            break_save = False\n",
    "                            break;\n",
    "\n",
    "                            \n",
    "                        if (break_save == True):\n",
    "                            break_save = False\n",
    "                            break;\n",
    "                        \n",
    "                        outline += ' f' + str(other1[0] )\n",
    "                        i += 1\n",
    "                        if i >= walk_len:\n",
    "                            break;\n",
    "                            \n",
    "                        pros2 = other_pro2[other1[0]]\n",
    "                        start = random.choices(pros2[:-1], pros2[-1])[0][0]\n",
    "\n",
    "                        outline += ' v' + str(start )\n",
    "                        i += 1\n",
    "                        if i >= walk_len:\n",
    "                            break;\n",
    "                            \n",
    "                        dc+=1\n",
    "                except KeyError as e:\n",
    "                    outfile.write(outline + \"\\n\")\n",
    "                    error_count += 1\n",
    "                    continue\n",
    "\n",
    "                outfile.write(outline + \"\\n\")\n",
    "\n",
    "        outfile.close()  \n",
    "        print('do_walks| errors : %d out of %d'%(error_count, dc))\n",
    "        print()\n",
    "        \n",
    "    def __get_node_groupby(self):\n",
    "        df_m = self.__df_merge\n",
    "        df_n = self.__df_nodes\n",
    "        cols = self.__merge_col_names\n",
    "#         ['h','t','r', 'h_id', 't_id', 'r_id']\n",
    "        \n",
    "        df_g_h_r = df_m.groupby([cols[3], cols[5]])[cols[4]].count().reset_index(name = cols[2] + '_count')\n",
    "        df_g_t_r = df_m.groupby([cols[4], cols[5]])[cols[3]].count().reset_index(name = cols[1] + '_count')\n",
    "        print('__get_node_groupby| df_g_h_r: {}, df_g_t_r: {}'.format(df_g_h_r.shape, df_g_t_r.shape))\n",
    "        \n",
    "        dict_t = dict()\n",
    "        dict_w = dict()\n",
    "        for i in df_g_h_r.itertuples():\n",
    "            h_id  = i[1]\n",
    "            r_id  = i[2]\n",
    "            count = i[3]\n",
    "            if h_id not in dict_w:\n",
    "                dict_w[h_id] = dict()\n",
    "\n",
    "            dict_w[h_id][r_id] = count\n",
    "            \n",
    "            if h_id not in dict_t:\n",
    "                dict_t[h_id] = 0\n",
    "            dict_t[h_id] += count\n",
    "\n",
    "        print('__get_node_groupby| dict_w after first columns: {}'.format(len(dict_w)))\n",
    "        print('__get_node_groupby| dict_t after first columns: {}'.format(len(dict_t)))\n",
    "\n",
    "        for i in df_g_t_r.itertuples():\n",
    "            t_id  = i[1]\n",
    "            r_id  = i[2]\n",
    "            count = i[3]\n",
    "            if t_id not in dict_w:\n",
    "                dict_w[t_id] = dict()\n",
    "\n",
    "                if r_id not in dict_w[t_id]:\n",
    "                    dict_w[t_id][r_id] = count\n",
    "                else:\n",
    "                    dict_w[t_id][r_id] += count\n",
    "            if t_id not in dict_t:\n",
    "                dict_t[t_id] = 0\n",
    "            dict_t[t_id] += count\n",
    "\n",
    "        print('__get_node_groupby| dict_w after second columns: {}'.format(len(dict_w)))\n",
    "        print('__get_node_groupby| dict_t after second columns: {}'.format(len(dict_t)))\n",
    "        self.__dict_w = dict_w\n",
    "        self.__dict_t = dict_t\n",
    "        print()\n",
    "        \n",
    "    def __set_biased(self):\n",
    "        d_bias = self.__d_bias\n",
    "        total_weights = 0\n",
    "        for k in d_bias:\n",
    "            total_weights += d_bias[k]\n",
    "\n",
    "        d_por = { k: d_bias[k] / total_weights for k in d_bias }\n",
    "        self.__d_por = d_por\n",
    "        \n",
    "    def __get_dist(self, lst):\n",
    "        d_dist = self.__d_por\n",
    "        \n",
    "        if type(lst[0])== tuple:\n",
    "            targets = list(list(zip(*lst))[0])\n",
    "        else:\n",
    "            targets = lst\n",
    "            \n",
    "        lst_weights = [ d_dist[self.__d_node_to_class[i]] if self.__d_node_to_class[i] in d_dist else 0  for i in targets]\n",
    "        sum_weights = sum(lst_weights)\n",
    "        \n",
    "        if sum_weights == 0:\n",
    "            self.zero_dist_len += 1\n",
    "            raise Error()\n",
    "        \n",
    "        nei_prob = [ lst_weights[w]/sum_weights for w in range(len(lst_weights)) if w != len(lst_weights)-1]\n",
    "        remained = 1 - sum(nei_prob)\n",
    "        nei_prob.append(remained)\n",
    "        \n",
    "        return nei_prob\n",
    "            \n",
    "    # used to be __get_dist_v2\n",
    "    def __get_dist_point(self, lst, lam=1):\n",
    "        d_t = self.__dict_t\n",
    "        if type(lst[0])== tuple:\n",
    "            targets = list(list(zip(*lst))[0])\n",
    "        else:\n",
    "            targets = lst\n",
    "\n",
    "        nei_degree =  [ d_t[i] for i in targets]\n",
    "        sum_degree = sum(nei_degree)\n",
    "        nei_point = [ sum_degree/p for p in nei_degree]\n",
    "        sum_point = sum(nei_point)\n",
    "\n",
    "        nei_prob = [ nei_point[w]/sum_point  for w in range(len(nei_point)) if w != len(nei_point)-1]\n",
    "        remained = 1 - sum(nei_prob)\n",
    "        nei_prob.append(remained)\n",
    "        return nei_prob\n",
    "        \n",
    "    def __get_dist_v1(self, lst, lam=1):\n",
    "        d_t = self.__dict_t\n",
    "        if type(lst[0])== tuple:\n",
    "            targets = list(list(zip(*lst))[0])\n",
    "        else:\n",
    "            targets = lst\n",
    "            \n",
    "        weights =  [ lam/d_t[i] for i in targets]\n",
    "        s = np.sum(weights)\n",
    "        prob = []\n",
    "        for i in range(len(weights)):\n",
    "            if i != len(weights)-1 :\n",
    "                prob.append(weights[i]/s)\n",
    "            else:\n",
    "                prob.append(1.0 - np.sum(prob[:i]))\n",
    "        return prob\n",
    "    \n",
    "    def __validate_all_dicts(self):\n",
    "        df_n   = self.__df_nodes\n",
    "        set_n  = set(df_n['id'])\n",
    "        print('validate_all_dicts| set of nodes: {}'.format(len(set_n)))\n",
    "        \n",
    "        pro_other  = self.__pro_other\n",
    "        other_pro  = self.__other_pro \n",
    "        pro_other2 = self.__pro_other2 \n",
    "        other_pro2 = self.__other_pro2\n",
    "        pro_path   = self.__pro_path\n",
    "        path_pro   = self.__path_pro\n",
    "        \n",
    "        pro_other_ks  = list(pro_other.keys())\n",
    "        other_pro_ks  = list(other_pro.keys())\n",
    "        pro_other2_ks = list(pro_other2.keys())\n",
    "        other_pro2_ks = list(other_pro2.keys())\n",
    "        pro_path_ks   = list(pro_path.keys())\n",
    "        path_pro_ks   = list(path_pro.keys())\n",
    "        \n",
    "        list_all_keys = pro_other_ks + other_pro_ks + pro_other2_ks + other_pro2_ks + pro_path_ks + path_pro_ks\n",
    "        set_all_keys = set(list_all_keys)\n",
    "        \n",
    "        print('validate_all_dicts| list_all_keys: {}, set_all_keys: {}'.format(len(list_all_keys), len(set_all_keys)))\n",
    "        \n",
    "        set_rest = set_n - set_all_keys\n",
    "        print('missing nodes from dicts: {}'.format(len(set_rest)))\n",
    "        \n",
    "        self.missing_nodes_from_dict = set_rest\n",
    "        \n",
    "        \n",
    "\n",
    "    def get__df_merge(self):\n",
    "        return self.__df_merge\n",
    "    \n",
    "    def get__df_nodes(self):\n",
    "        return self.__df_nodes\n",
    "    \n",
    "    def get__df_relations(self):\n",
    "        return self.__df_relations\n",
    "    \n",
    "    def get__dict_w(self):\n",
    "        return self.__dict_w\n",
    "    \n",
    "    def get__dict_t(self):\n",
    "        return self.__dict_t\n",
    "    \n",
    "    def get__pro_other(self):\n",
    "        return self.__pro_other\n",
    "    \n",
    "    def get__pro_path (self):\n",
    "        return self.__pro_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__read_processed_files| df_merge: (2851116, 6)\n",
      "__read_processed_files| df_nodes: (212557, 2)\n",
      "__read_processed_files| df_relations: (7, 2)\n",
      "\n",
      "__get_node_groupby| df_g_h_r: (466654, 3), df_g_t_r: (53540, 3)\n",
      "__get_node_groupby| dict_w after first columns: 178320\n",
      "__get_node_groupby| dict_t after first columns: 178320\n",
      "__get_node_groupby| dict_w after second columns: 212557\n",
      "__get_node_groupby| dict_t after second columns: 212557\n",
      "\n",
      "__load_node_to_class| __d_node_to_class: 212557\n",
      "__prepare_dict_pro_path| df_hasPathway: (135206, 3)\n",
      "__prepare_dict_pro_path| pro_path: 56767, path_pro: 1584\n",
      "\n",
      "__prepare_dict_pro_x| df_other: (2715910, 3)\n",
      "__prepare_dict_pro_x| processing the other to protein (other_pro) ...\n",
      "__prepare_dict_pro_x| lst_pro_path_Proteins: 56767\n",
      "__prepare_dict_pro_x| df_others_short_selected: (1403451, 3)\n",
      "__prepare_dict_pro_x| other_pro: 43232\n",
      "__prepare_dict_pro_x| processing the protein to other (pro_other) ...\n",
      "__prepare_dict_pro_x| lst_other_pro_Others: 43232\n",
      "__prepare_dict_pro_x| df_others_short_selected_others: (2645994, 3)\n",
      "__prepare_dict_pro_x| pro_other: 169660\n",
      "__prepare_dict_pro_x| The Final dict: pro_other: 169660, other_pro: 43232\n",
      "__prepare_dict_pro_x| processing the protein to other 2 (pro_other2) ...\n",
      "__prepare_dict_pro_x| lst_path_pro_Proteins: 135206\n",
      "__prepare_dict_pro_x| df_shorts_short_seleected_proteins2: (1403451, 3)\n",
      "__prepare_dict_pro_x| pro_other2: 56346\n",
      "__prepare_dict_pro_x| processing the other to protein 2 (other_pro2) ...\n",
      "__prepare_dict_pro_x| lst_pro_other2_Others: 1403451\n",
      "__prepare_dict_pro_x| df_shorts_short_seleected_others2: (2645994, 3)\n",
      "__prepare_dict_pro_x| other_pro2: 43232\n",
      "__prepare_dict_pro_x| The Final dict: pro_other2: 56346, other_pro2: 43232\n",
      "\n",
      "__change_dict_value_type| started.\n",
      "49077 out of 169660\n",
      "0 out of 43232\n",
      "10399 out of 56346\n",
      "0 out of 43232\n",
      "0 out of 56767\n",
      "0 out of 1584\n",
      "__change_dict_value_type| done.\n",
      "\n",
      "__prepare_dicts| self.zero_dist_len: 59476\n",
      "validate_all_dicts| set of nodes: 212557\n",
      "validate_all_dicts| list_all_keys: 311345, set_all_keys: 160989\n",
      "missing nodes from dicts: 51568\n"
     ]
    }
   ],
   "source": [
    "d_bias = { 'BP': 1, 'CC':1, 'MF': 2}\n",
    "w = WalkBiased(data_path, 'df_merge_cc_train', 'df_nodes_cc', 'df_relations', 'dict_node_to_class', d_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 115710/120583 [09:42<00:25, 192.87it/s]"
     ]
    }
   ],
   "source": [
    "w.do_biased_walks(40, 40, desc = '-biased-v4_BP1-CC1-MF2-v2', seed = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - BP1-CC1-MF2: \n",
    "   - __prepare_dicts| self.zero_dist_len: 204291\n",
    "   - do_walks| errors : 225246 out of 39528297\n",
    " - BP1:\n",
    "   - __prepare_dicts| self.zero_dist_len: 232622\n",
    "   - do_walks| errors : 202709 out of 39479486\n",
    " - BP1-v2:\n",
    "   - __prepare_dicts| self.zero_dist_len: 87807\n",
    "   - do_walks| errors : 3840522 out of 4657056\n",
    " - BP1-CC1-MF2-v2: \n",
    "   - __prepare_dicts| self.zero_dist_len: 59476\n",
    "   - \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- number of walks 150, len:40, seed = 2: \n",
    "  - 08:50 -- 10:13(50%) --> 11:41\n",
    "- number of walks 100, len:80, seed = 2:\n",
    "  - 13:40 -- 14:30(20%) --> 16:21(70%) -- 17:28\n",
    "- number of walks 100, len:40, seed = 71\n",
    "  - 09:30 --> 11:13\n",
    "- number of walks 40, len:40, seed = 2\n",
    "  - 15:12 --> 15:54\n",
    "- number of walks:1, len: 10, seed = 20\n",
    "  - 17s\n",
    "- number of walks:1, len:5, seed = 20\n",
    "  - 8s\n",
    "  \n",
    "- REAL Bised 100, 40, 5\n",
    "  - started: 10:22 -- 20% 10:38 --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./metapath2vec -train graphpattern2vec_same/data/run-11/walks-pro-x-pro-path-100-40-graphpattern2vec-biased-v2-rewire-merge_cc_train_same_run11           -output graphpattern2vec_same/data/run-11/emb-pro-x-pro-path-100-40-graphpattern2vec-biased-v2-rewire-merge_cc_train_same_run11                      -pp 1 -size 128 -window 7 -negative 5 -threads 32\n",
    "\n",
    "# ./metapath2vec -train graphpattern2vec_walk/data/walks-pro-x-pro-path-100-40-graphpattern2vec-biased-v2-rewire-merge_cc_train_while_loop_no_raise_all_pro -output graphpattern2vec_walk/data/emb-pro-x-pro-path-100-40-graphpattern2vec-biased-v2-rewire-merge_cc_train_while_loop_no_raise_all_pro            -pp 1 -size 128 -window 7 -negative 5 -threads 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_biased(d_bias):\n",
    "\n",
    "    total_weights = 0\n",
    "    for k in d_bias:\n",
    "        total_weights += d_bias[k]\n",
    "        \n",
    "    d_por = { k: d_bias[k] / total_weights for k in d_bias }\n",
    "    print(d_por)\n",
    "    \n",
    "    \n",
    "d_bias = { 'BP': 1, 'CC': 1, 'MF': 2}\n",
    "set_biased(d_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Walks for rewire graph (I don't do rewireing for this version. rewireing done in gp2v_rewire package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itr = 2\n",
    "# w2 = WalkBiased(data_path, 'df_merge_rew_train-NoSplit_{}'.format(itr), 'df_nodes_cc', 'df_relations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2.do_biased_walks(100, 40, desc = '-biased-v2-rewire-merge_rew_train_NoSplit_{}'.format(itr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHGO          : do_walks| errors : 2729941 out of 108559699\n",
    "# PATHGO-bised-v2 : do_walks| errors :  906173 out of 116408720\n",
    "# rewire:         : do_walks| errors : 1029512 out of 115184002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ./metapath2vec -train graphpattern2vec_rewire/data/walks-pro-x-pro-path-100-40-graphpattern2vec-biased-v2-rewire-merge_rew_train_NoSplit_1 -output graphpattern2vec_rewire/data/emb-pro-x-pro-path-100-40-graphpattern2vec-biased-v2-rewire-merge_rew_train_NoSplit_1 -pp 1 -size 128 -window 7 -negative 5 -threads 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./metapath2vec -train graphpattern2vec_rewire/data/walks-pro-x-pro-path-100-40-graphpattern2vec-biased-v2-rewire-merge_rew_train -output graphpattern2vec_rewire/data/emb-pro-x-pro-path-100-40-graphpattern2vec-biased-v2-rewire-merge_rew_train -pp 1 -size 128 -window 7 -negative 5 -threads 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows the new Walk with possibility of v-v-v-f-v-a is working\n",
    "# it seems that (6931825, 21213)  less than 0.1% of the time compare to all adjacent tokens\n",
    "# if devided by 5 : 21213/1386365 1% of time\n",
    "\n",
    "# check the previous walks...-fixed when ppi does not apeasr in the walks at all\n",
    "\n",
    "\n",
    "\n",
    "# def get_dist(lst,dict_t, lam=0.5):\n",
    "#     d_t = dict_t\n",
    "    \n",
    "#     if type(lst[0])== tuple:\n",
    "#         targets = list(list(zip(*lst))[0])\n",
    "#     else:\n",
    "#         targets = lst\n",
    "\n",
    "#     weights =  [ lam/d_t[i] for i in targets]\n",
    "#     s = np.sum(weights)\n",
    "#     prob = []\n",
    "    \n",
    "#     for i in range(len(weights)):\n",
    "#         if i != len(weights)-1 :\n",
    "#             prob.append(weights[i]/s)\n",
    "#         else:\n",
    "#             prob.append(1.0 - np.sum(prob[:i]))\n",
    "            \n",
    "#     return prob\n",
    "\n",
    "# def dist(lst, dict_t):\n",
    "#     sum_degree = 0\n",
    "#     for i in dict_t:\n",
    "#         sum_degree += dict_t[i]\n",
    "#     print('sum degree: {}'.format(sum_degree))\n",
    "    \n",
    "#     for i in lst:\n",
    "#         points = [ sum_degree/dict_t[i] for i in dict_t]\n",
    "#     print('points: {}'.format(points))    \n",
    "#     sum_point = sum(points)\n",
    "#     print('sum_point: {}'.format(sum_point))    \n",
    "    \n",
    "#     nei_prob = [ points[w]/sum_point  for w in range(len(points)) if w != len(points)-1]\n",
    "#     remained = 1 - sum(nei_prob)\n",
    "#     nei_prob.append(remained)\n",
    "#     if (sum(nei_prob) != 1):\n",
    "#         print('error: {}'.format(nei_prob))\n",
    "#     return nei_prob\n",
    "# lst = ['a','b','c']\n",
    "# dict_t = {'a':10, 'b':1, 'c':10000}\n",
    "# for i in [0.005, 0.05, 0.5]:\n",
    "#     print(get_dist(lst, dict_t, lam=i))\n",
    "    \n",
    "# # d = dist(lst, dict_t)\n",
    "# # print(d)\n",
    "# c = 0\n",
    "# l = list()\n",
    "# for line in tqdm(open('data/walks-pro-x-pro-path-100-40-graphpattern2vec-HL-Path-Biased-fixed' , 'r')):\n",
    "#     w = line.strip().split(' ')\n",
    "#     prev = ''\n",
    "#     prev_item = ''\n",
    "#     for i in w:\n",
    "#         c+=1\n",
    "#         if prev == i[0] == 'v':\n",
    "#             l.append([i, prev_item])\n",
    "#         prev = i[0]\n",
    "#         prev_item = i\n",
    "# c, len(l)\n",
    "\n",
    "# shows the new Walk with possibility of v-v-v-f-v-a is working\n",
    "# it seems that (6931825, 21213)  less than 0.1% of the time compare to all adjacent tokens\n",
    "# if devided by 5 : 21213/1386365 1% of time\n",
    "\n",
    "# check the previous walks...-fixed when ppi does not appeasr in the walks at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def Mafia_roles():\n",
    "    p = ['Navid', 'Abbas', 'Mehdi', 'Sanaz hagh', 'Arash', 'Sanaz Sattar', 'Soroush', 'Elham', 'saber', 'Mona']\n",
    "    r = ['Mafia', 'Mafia', 'God Father', 'Karagah', 'Doctor','shahrvand','shahrvand','shahrvand','shahrvand','shahrvand']\n",
    "    if len(p) == len(r):\n",
    "        sel_p = random.sample(p, len(p))\n",
    "        sel_r = random.sample(r, len(r))\n",
    "        final = list(zip (sel_p, sel_r))\n",
    "        \n",
    "        print(final)\n",
    "    else:\n",
    "        print('lists had different length')\n",
    "\n",
    "Mafia_roles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def validate_all_dicts(self):\n",
    "        df_n   = self.__df_nodes\n",
    "        set_n  = set(df_n['id'])\n",
    "        print('validate_all_dicts| set of nodes: {}'.format(len(set_n)))\n",
    "        \n",
    "        pro_other  = self.__pro_other\n",
    "        other_pro  = self.__other_pro \n",
    "        pro_other2 = self.__pro_other2 \n",
    "        other_pro2 = self.__other_pro2\n",
    "        pro_path   = self.__pro_path\n",
    "        path_pro   = self.__path_pro\n",
    "        \n",
    "        pro_other_ks  = list(pro_other.keys())\n",
    "        other_pro_ks  = list(other_pro.keys())\n",
    "        pro_other2_ks = list(pro_other2.keys())\n",
    "        other_pro2_ks = list(other_pro2.keys())\n",
    "        pro_path_ks   = list(pro_path.keys())\n",
    "        path_pro_ks   = list(path_pro.keys())\n",
    "        \n",
    "        list_all_keys = pro_other_ks + other_pro_ks + pro_other2_ks + other_pro2_ks + pro_path_ks + path_pro_ks\n",
    "        set_all_keys = set(list_all_keys)\n",
    "        \n",
    "        print('validate_all_dicts| list_all_keys: {}, set_all_keys: {}'.format(len(list_all_keys), len(set_all_keys)))\n",
    "        \n",
    "        set_rest = set_n - set_all_keys\n",
    "        print('missing nodes from dicts: {}'.format(len(set_rest)))\n",
    "        \n",
    "        self.missing_nodes_from_dict = set_rest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
