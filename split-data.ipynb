{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(filename, merge=False):\n",
    "    if merge == True:\n",
    "        df = pd.read_csv(path + filename, dtype={'t':str})\n",
    "    else:    \n",
    "        df = pd.read_csv(path + filename)\n",
    "    print('read_files| read file {} with shape {}'.format(filename, df.shape))\n",
    "    print()\n",
    "    return df\n",
    "\n",
    "def get_df_by_relation(df_m, list_r):\n",
    "    df = df_m.loc[df_m['r'].isin(list_r)]\n",
    "    print('get_df_by_relation| df: {}'.format(df.shape))\n",
    "    return df\n",
    "\n",
    "def get_list_kinase(filename):\n",
    "    df = pd.read_csv(path + filename)\n",
    "    df = df.rename(columns = {'uniprot':'id'})\n",
    "\n",
    "    lst= list(df['id'].unique())\n",
    "    print('get_list_kinase| df: {}, list: {}'.format(len(lst), df.shape))\n",
    "    print()\n",
    "    \n",
    "    return lst, df\n",
    "   \n",
    "def filter_merge_by_head_N_relation(df_m, list_h, list_r):\n",
    "    df_sel = df_m.loc[(df_m['h'].isin(list_h)) & (df_m['r'].isin(list_r))]\n",
    "    print('filter_merge_by_head_N_relation| df_sel: {}'.format(df_sel.shape))\n",
    "    \n",
    "def df_to_dict(df):\n",
    "    d = dict()\n",
    "    for i in df.itertuples():\n",
    "        \n",
    "        h_id = i[1]\n",
    "        t_id = i[2]\n",
    "        \n",
    "        if h_id not in d:\n",
    "            d[h_id] = set()\n",
    "        d[h_id].add(t_id)\n",
    "        \n",
    "        if t_id not in d:\n",
    "            d[t_id] = set()\n",
    "        d[t_id].add(h_id)\n",
    "    print('df_to_dict| d:{}'.format(len(d)))\n",
    "    return d\n",
    "\n",
    "def randomly_sample_df(df, rnd_state = 1, split_ratio = 3):\n",
    "    split_number = int(df.shape[0] / split_ratio)\n",
    "    \n",
    "    df_short = df[['h_id', 't_id', 'r_id']]\n",
    "    d = df_to_dict(df_short)\n",
    "    \n",
    "    idx_list = df_short.index\n",
    "    idx = random.choices(idx_list, k = split_number)\n",
    "    \n",
    "    selected_idx = list()\n",
    "    \n",
    "    for i in tqdm(range(len(idx))):\n",
    "        \n",
    "        res = df_short.iloc[i, [0, 1]].to_numpy()\n",
    "        h_id = res[0]\n",
    "        t_id = res[1]\n",
    "        \n",
    "        if (len(d[h_id]) > 1) and (len(d[t_id]) > 1) :\n",
    "            try:\n",
    "                d[h_id].remove(t_id)\n",
    "                d[t_id].remove(h_id)\n",
    "                selected_idx.append(i)\n",
    "            except KeyError:\n",
    "                print(idx, h_id, t_id, d[h_id], d[t_id])\n",
    "                \n",
    "    print('randomly_sample_df| delete dictionary.')        \n",
    "    del(d)\n",
    "    \n",
    "    print('randomly_sample_df| selected_idx: {}, split_number: {}'.format(len(selected_idx), split_number))\n",
    "    \n",
    "    df_sample = df.iloc[selected_idx]\n",
    "    return df_sample\n",
    "    \n",
    "def split_df(df,df_sample):\n",
    "    sample_index = df_sample.index\n",
    "    df_rest = df.loc[~df.index.isin(sample_index)]\n",
    "    print('split_df| df_rest: {}, df_sample: {}, df: {}'.format(df_rest.shape, df_sample.shape, df.shape))\n",
    "    return df_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_process(edge_file, node_file, relation_file, light_file, dark_file, train_file, test_file):\n",
    "    df_merge      = read_files(edge_file, merge=True)\n",
    "    df_nodes      = read_files(node_file)\n",
    "    df_relations  = read_files(relation_file)\n",
    "\n",
    "    print('split_process| df_merge: {}, df_nodes: {}, df_relations: {}'.format(df_merge.shape, df_nodes.shape, df_relations.shape))\n",
    "\n",
    "    list_light, df_light = get_list_kinase(light_file)\n",
    "    list_dark, df_dark = get_list_kinase(dark_file)\n",
    "\n",
    "    df_light_pathway = filter_merge_by_head_N_relation(df_merge, list_light, ['hasPathway'])\n",
    "    df_dark_pathway = filter_merge_by_head_N_relation(df_merge, list_dark, ['hasPathway'])\n",
    "\n",
    "    df_hasPathway = get_df_by_relation(df_merge, ['hasPathway'])\n",
    "\n",
    "    df_sample = randomly_sample_df(df_hasPathway)\n",
    "    df_rest = split_df(df_merge, df_sample)\n",
    "\n",
    "    print('split_process| saving...')\n",
    "    df_sample.to_csv(path + test_file, index=False)\n",
    "    df_rest.to_csv(path + train_file, index=False)\n",
    "    print('split_process| saving done.')\n",
    "    \n",
    "    return df_sample, df_rest, df_merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges_by_relation(df, r):\n",
    "    return df[df['r'] == r]\n",
    "\n",
    "def get_column_uniq(df, r):\n",
    "    return set(df[r].unique())\n",
    "\n",
    "def check_nodes_appears_in_both_splits(df_m, df_s):\n",
    "    df_m_hasPathway = get_edges_by_relation(df_m, 'hasPathway')\n",
    "    set_m_pathways = get_column_uniq(df_m_hasPathway, 't')\n",
    "    set_m_proteins = get_column_uniq(df_m_hasPathway, 'h')\n",
    "\n",
    "    df_s_hasPathway = get_edges_by_relation(df_s, 'hasPathway')\n",
    "    set_s_pathways = get_column_uniq(df_s_hasPathway, 't')\n",
    "    set_s_proteins = get_column_uniq(df_s_hasPathway, 'h')\n",
    "    \n",
    "    print('check_nodes_appears_in_both_splits| set_m_pathways: {}, set_s_pathways: {}'.format(len(set_m_pathways), len(set_s_pathways)))\n",
    "    print('check_nodes_appears_in_both_splits| number of common nodes: {}'.format(len(set_m_pathways & set_s_pathways)))\n",
    "    print()\n",
    "    \n",
    "    print('check_nodes_appears_in_both_splits| set_m_proteins: {}, set_s_proteins: {}'.format(len(set_m_proteins), len(set_s_proteins)))\n",
    "    print('check_nodes_appears_in_both_splits| number of common nodes: {}'.format(len(set_m_proteins & set_s_proteins)))\n",
    "    # df_s['r'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Original Graph, Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_files| read file df_merge_cc.csv with shape (2886875, 6)\n",
      "\n",
      "read_files| read file df_nodes_cc.csv with shape (212557, 2)\n",
      "\n",
      "read_files| read file df_relations.csv with shape (7, 2)\n",
      "\n",
      "split_process| df_merge: (2886875, 6), df_nodes: (212557, 2), df_relations: (7, 2)\n",
      "get_list_kinase| df: 385, list: (385, 3)\n",
      "\n",
      "get_list_kinase| df: 160, list: (160, 3)\n",
      "\n",
      "filter_merge_by_head_N_relation| df_sel: (1725, 6)\n",
      "filter_merge_by_head_N_relation| df_sel: (136, 6)\n",
      "get_df_by_relation| df: (170965, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1173/56988 [00:00<00:09, 5884.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_to_dict| d:58351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56988/56988 [00:09<00:00, 5866.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomly_sample_df| delete dictionary.\n",
      "randomly_sample_df| selected_idx: 35759, split_number: 56988\n",
      "split_df| df_rest: (2851116, 6), df_sample: (35759, 6), df: (2886875, 6)\n",
      "split_process| saving...\n",
      "split_process| saving done.\n",
      "check_nodes_appears_in_both_splits| set_m_pathways: 1584, set_s_pathways: 892\n",
      "check_nodes_appears_in_both_splits| number of common nodes: 892\n",
      "\n",
      "check_nodes_appears_in_both_splits| set_m_proteins: 56767, set_s_proteins: 9470\n",
      "check_nodes_appears_in_both_splits| number of common nodes: 9470\n"
     ]
    }
   ],
   "source": [
    "df_s, df_rest,df_m = split_process('df_merge_cc.csv', 'df_nodes_cc.csv', 'df_relations.csv', 'light_kinase.csv', 'dark_kinase.csv', 'df_merge_cc_train.csv','df_merge_cc_test.csv')\n",
    "check_nodes_appears_in_both_splits(df_m, df_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2851116, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(df):\n",
    "    \n",
    "    print('load_graph| df : {}'.format(df.shape))\n",
    "    df_edges = df[['h_id', 't_id','r_id']]\n",
    "    edges = [tuple(x) for x in df_edges.values]\n",
    "    print('load_graph| edges: {}'.format(len(edges)))\n",
    "\n",
    "    GRAPH = nx.Graph()   # or DiGraph, MultiGraph, MultiDiGraph, etc\n",
    "    GRAPH.add_weighted_edges_from(edges)\n",
    "    return GRAPH\n",
    "\n",
    "def Connected_component(G):\n",
    "    connected_components_list_len = [len(c) for c in sorted(nx.connected_components(G), key=len, reverse=True)]\n",
    "    print('__check_connectivity| number of connected components: {}'.format(len(connected_components_list_len)))\n",
    "    cc_g = list( nx.connected_components(G) )\n",
    "    total_from_cc = np.sum(connected_components_list_len) # IT WAS OKAY\n",
    "    print('__check_connectivity| number of total node in data based on the Networkx\\'s Graph {}'.format(total_from_cc) )\n",
    "    return cc_g, connected_components_list_len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_graph| df : (2851116, 6)\n",
      "load_graph| edges: 2851116\n",
      "__check_connectivity| number of connected components: 1\n",
      "__check_connectivity| number of total node in data based on the Networkx's Graph 212557\n"
     ]
    }
   ],
   "source": [
    "g_train = load_graph(df_rest)\n",
    "cc_g, connected_components_list_len = Connected_component(g_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check reachable Nodes cause the 196430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_cc_train = pd.read_csv('data/df_merge_cc_train.csv')\n",
    "df_merge_cc_test = pd.read_csv('data/df_merge_cc_test.csv')\n",
    "df_nodes_cc = pd.read_csv('data/df_nodes_cc.csv')\n",
    "df_nodes = pd.read_csv('data/df_nodes.csv') \n",
    "print('df_merge_cc_train: {}, df_merge_cc_test: {}, df_nodes_cc: {}, df_nodes: {}'.format(df_merge_cc_train.shape, df_merge_cc_test.shape, df_nodes_cc.shape, df_nodes.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{type(i) for i in df_merge_cc_train['t'] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_lst = list(df_merge_cc_train['h'].unique())\n",
    "t_lst = list(df_merge_cc_train['t'].unique())\n",
    "\n",
    "nodes_train = list(set(h_lst + t_lst))\n",
    "set_nodes_train = set(nodes_train)\n",
    "print('nodes_train: {}'.format(len(nodes_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_lst2 = list(df_merge_cc_test['h'].unique())\n",
    "t_lst2 = list(df_merge_cc_test['t'].unique())\n",
    "t_lst2 = [str(i) for i in t_lst2]\n",
    "print({type(i) for i in t_lst2})\n",
    "\n",
    "nodes_test = list(set(h_lst2+ t_lst2))\n",
    "print('nodes_test: {}'.format(len(nodes_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in nodes_test:\n",
    "    if i not in set_nodes_train:\n",
    "        c+=1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_cc = list(df_nodes_cc['name'])\n",
    "set_nodes_cc = set(nodes_cc)\n",
    "len(nodes_cc), len(set_nodes_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nodes_test:\n",
    "    if i not in set_nodes_cc:\n",
    "        c+=1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = list(df_nodes['name'])\n",
    "set_nodes = set(nodes)\n",
    "len(nodes), len(set_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nodes_test:\n",
    "    if i not in set_nodes:\n",
    "        c+=1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "212557 - 196431"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = dict()\n",
    "for i in df_merge_cc_train.itertuples():\n",
    "    h = i[1]\n",
    "    t = i[2]\n",
    "    \n",
    "    if h not in d_train:\n",
    "        d_train[h] = 0\n",
    "    d_train[h] += 1\n",
    "    \n",
    "    if t not in d_train:\n",
    "        d_train[t] = 0\n",
    "    d_train[t] +=1\n",
    "len(d_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for d in d_train:\n",
    "    if d_train[d] == 4:\n",
    "        c+=1\n",
    "c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = set()\n",
    "for d in d_train:\n",
    "    st.add(d_train[d])\n",
    "np.std(list(st)), np.mean(list(st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py37] *",
   "language": "python",
   "name": "conda-env-py37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
